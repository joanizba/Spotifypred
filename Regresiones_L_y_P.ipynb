{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joanizba/Spotifypred/blob/dev_joan/Regresiones_L_y_P.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNb9FOB7MnnN",
        "outputId": "965a8498-7cb2-42b2-d66f-7bafe72a280e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n",
        "file_path ='/content/drive/MyDrive/PROYECTO_FINAL/dataset/playlist_2010to2022.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIGONuaqgccO"
      },
      "source": [
        "### **Regresión lineal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tnIG9B0-bJp",
        "outputId": "eb04a675-8084-4b9d-87b0-0af362c85192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos cargados exitosamente desde '/content/drive/MyDrive/PROYECTO_FINAL/dataset/playlist_2010to2022.csv'\n",
            "Número de filas: 2300, Número de columnas: 23\n",
            "\n",
            "Valores faltantes antes de limpiar:\n",
            "year                 0\n",
            "track_popularity     0\n",
            "artist_popularity    0\n",
            "danceability         1\n",
            "energy               1\n",
            "key                  1\n",
            "loudness             1\n",
            "mode                 1\n",
            "speechiness          1\n",
            "acousticness         1\n",
            "instrumentalness     1\n",
            "liveness             1\n",
            "valence              1\n",
            "tempo                1\n",
            "duration_ms          1\n",
            "time_signature       1\n",
            "dtype: int64\n",
            "\n",
            "Forma del dataset después de limpiar NaNs: (2299, 16)\n",
            "\n",
            "Características seleccionadas (X):\n",
            "['year', 'artist_popularity', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature']\n",
            "\n",
            "Variable objetivo (y): track_popularity\n",
            "\n",
            "Tamaño del conjunto de entrenamiento: 1839 muestras\n",
            "Tamaño del conjunto de prueba: 460 muestras\n",
            "\n",
            "Datos normalizados (primeras 5 filas de entrenamiento):\n",
            "[[ 2.95986640e-03  1.15866300e+00 -1.27093607e-01 -2.43076953e+00\n",
            "   1.04241393e+00 -1.95671261e+00  8.29882663e-01 -6.60894764e-01\n",
            "   2.48089284e+00 -1.65442679e-01  5.89119277e-02 -5.65311438e-01\n",
            "  -1.28116204e+00  7.86743523e-01  7.56519983e-02]\n",
            " [-1.20663887e+00 -7.17886584e-01 -9.87529806e-02 -5.44883850e-01\n",
            "   1.04241393e+00 -3.63243251e-01  8.29882663e-01  3.86627487e+00\n",
            "   3.45838582e-01 -1.65442679e-01 -7.14224692e-01  1.21797096e+00\n",
            "   2.53135858e+00 -3.93123990e-02  7.56519983e-02]\n",
            " [-2.99439817e-01  9.95484777e-01 -2.35891790e+00 -6.17651143e-01\n",
            "  -3.35334539e-01 -3.83227042e-01  8.29882663e-01 -6.94206480e-01\n",
            "  -3.73645455e-01 -1.65442679e-01 -5.25167427e-01 -9.09608535e-01\n",
            "   1.42739878e+00  2.51103889e-01  7.56519983e-02]\n",
            " [ 1.21255860e+00 -3.91530135e-01  1.17657519e+00  5.04177956e-01\n",
            "   1.59351332e+00  7.55373193e-01 -1.20498963e+00 -4.92187043e-01\n",
            "   7.94297466e-01 -1.65442679e-01 -2.25442495e-01  9.97267696e-01\n",
            "   2.10686112e-01  3.02324159e-01  7.56519983e-02]\n",
            " [-1.35783871e+00  5.87539215e-01 -2.97137362e-01  1.50472823e+00\n",
            "   1.04241393e+00  1.12459751e+00  8.29882663e-01 -5.65257903e-01\n",
            "  -6.92928683e-01 -1.65442679e-01 -4.09963830e-02  1.50047115e+00\n",
            "  -1.05120602e+00  1.04193237e+00  7.56519983e-02]]\n",
            "\n",
            "--- Buscando el mejor alpha para Ridge ---\n",
            "Alphas a probar: [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "  Alpha=2.0000 -> R² = 0.0833, MSE = 148.2062\n",
            "  Alpha=3.0000 -> R² = 0.0833, MSE = 148.2033\n",
            "  Alpha=4.0000 -> R² = 0.0833, MSE = 148.2003\n",
            "  Alpha=5.0000 -> R² = 0.0833, MSE = 148.1974\n",
            "  Alpha=6.0000 -> R² = 0.0834, MSE = 148.1945\n",
            "  Alpha=7.0000 -> R² = 0.0834, MSE = 148.1916\n",
            "  Alpha=8.0000 -> R² = 0.0834, MSE = 148.1887\n",
            "  Alpha=9.0000 -> R² = 0.0834, MSE = 148.1859\n",
            "  Alpha=10.0000 -> R² = 0.0834, MSE = 148.1830\n",
            "\n",
            "Mejor alpha para Ridge: 10\n",
            "  Mejor R²: 0.0834\n",
            "  Mejor MSE: 148.1830\n",
            "\n",
            "--- Buscando el mejor alpha para Lasso ---\n",
            "Alphas a probar: [0.1, 1, 10]\n",
            "  Alpha=0.1000 -> R² = 0.0860, MSE = 147.7725\n",
            "  Alpha=1.0000 -> R² = 0.0807, MSE = 148.6176\n",
            "  Alpha=10.0000 -> R² = -0.0000, MSE = 161.6703\n",
            "\n",
            "Mejor alpha para Lasso: 0.1\n",
            "  Mejor R²: 0.0860\n",
            "  Mejor MSE: 147.7725\n",
            "\n",
            "--- Resumen de los Mejores Modelos Regularizados ---\n",
            "Mejor Ridge: Alpha=10, R²=0.0834, MSE=148.1830\n",
            "Mejor Lasso: Alpha=0.1, R²=0.0860, MSE=147.7725\n",
            "\n",
            "--- Evaluando Regresión Lineal Simple (Baseline) ---\n",
            "Regresión Lineal Simple: R²=0.0832, MSE=148.2121\n",
            "\n",
            "--- Coeficientes del Mejor Modelo Lasso (Alpha=0.1) ---\n",
            "       Característica  Coeficiente\n",
            "1   artist_popularity     3.692426\n",
            "0                year     0.827557\n",
            "8        acousticness     0.274517\n",
            "2        danceability     0.062658\n",
            "6                mode     0.047263\n",
            "9    instrumentalness     0.039787\n",
            "3              energy    -0.032012\n",
            "11            valence    -0.036822\n",
            "13        duration_ms    -0.055023\n",
            "14     time_signature    -0.194335\n",
            "10           liveness    -0.239820\n",
            "4                 key    -0.337867\n",
            "7         speechiness    -0.461150\n",
            "\n",
            "--- Coeficientes del Mejor Modelo Ridge (Alpha=10) ---\n",
            "       Característica  Coeficiente\n",
            "1   artist_popularity     3.756484\n",
            "0                year     0.795211\n",
            "8        acousticness     0.369120\n",
            "2        danceability     0.267338\n",
            "5            loudness     0.259290\n",
            "9    instrumentalness     0.160710\n",
            "6                mode     0.139826\n",
            "12              tempo     0.113567\n",
            "13        duration_ms    -0.145436\n",
            "3              energy    -0.189338\n",
            "11            valence    -0.197581\n",
            "14     time_signature    -0.287360\n",
            "10           liveness    -0.294238\n",
            "4                 key    -0.430665\n",
            "7         speechiness    -0.552178\n",
            "\n",
            "¡Análisis completado!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "\n",
        "# Ignorar advertencias para mantener la salida limpia (opcional)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. Carga de Datos ---\n",
        "# Asegúrate de que tu archivo CSV esté en tu Google Drive o súbelo a Colab\n",
        "# Si está en Drive, necesitas montar tu Drive primero:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# file_path = '/content/drive/My Drive/tu_carpeta/tu_dataset.csv' # Cambia esto a la ruta correcta\n",
        "\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "df = data.copy()\n",
        "print(f\"Datos cargados exitosamente desde '{file_path}'\")\n",
        "print(f\"Número de filas: {df.shape[0]}, Número de columnas: {df.shape[1]}\")\n",
        "\n",
        "# --- 2. Selección de Características y Preprocesamiento ---\n",
        "\n",
        "# Seleccionar características numéricas relevantes para la predicción\n",
        "# Excluimos IDs, nombres, URLs y géneros (que requerirían un preprocesamiento más complejo)\n",
        "# También excluimos el target 'track_popularity' de las features X\n",
        "numeric_features = [\n",
        "    'year', 'track_popularity', 'artist_popularity', 'danceability',\n",
        "    'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
        "    'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms',\n",
        "    'time_signature'\n",
        "]\n",
        "\n",
        "# Asegurarnos de que solo trabajamos con columnas que existen y son numéricas\n",
        "existing_features = [col for col in numeric_features if col in df.columns]\n",
        "df_subset = df[existing_features].copy()\n",
        "\n",
        "# Convertir columnas a numéricas si es necesario (forzando errores a NaN)\n",
        "for col in existing_features:\n",
        "    df_subset[col] = pd.to_numeric(df_subset[col], errors='coerce')\n",
        "\n",
        "# Manejar valores faltantes (una estrategia simple: eliminar filas con NaN en las columnas seleccionadas)\n",
        "print(f\"\\nValores faltantes antes de limpiar:\\n{df_subset.isnull().sum()}\")\n",
        "df_subset.dropna(inplace=True)\n",
        "print(f\"\\nForma del dataset después de limpiar NaNs: {df_subset.shape}\")\n",
        "\n",
        "if df_subset.empty:\n",
        "    print(\"Error: El dataset quedó vacío después de eliminar filas con valores faltantes.\")\n",
        "    exit()\n",
        "\n",
        "# Separar características (X) y variable objetivo (y)\n",
        "X = df_subset.drop('track_popularity', axis=1)\n",
        "y = df_subset['track_popularity']\n",
        "\n",
        "# Verificar que aún tenemos datos\n",
        "if X.empty or y.empty:\n",
        "    print(\"Error: No quedan datos suficientes después de la selección de características y limpieza.\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nCaracterísticas seleccionadas (X):\")\n",
        "print(X.columns.tolist())\n",
        "print(\"\\nVariable objetivo (y): track_popularity\")\n",
        "\n",
        "# --- 3. División en Conjuntos de Entrenamiento y Prueba ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"\\nTamaño del conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
        "print(f\"Tamaño del conjunto de prueba: {X_test.shape[0]} muestras\")\n",
        "\n",
        "# --- 4. Normalización (Escalado) de Datos ---\n",
        "# Es crucial escalar DESPUÉS de dividir para evitar fuga de datos del test set al scaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nDatos normalizados (primeras 5 filas de entrenamiento):\")\n",
        "print(X_train_scaled[:5])\n",
        "\n",
        "# --- 5. Función para Encontrar el Mejor Alpha ---\n",
        "def find_best_alpha(model_type, alphas, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Entrena modelos Ridge o Lasso con diferentes alphas y encuentra el mejor.\n",
        "\n",
        "    Args:\n",
        "        model_type (str): 'ridge' o 'lasso'.\n",
        "        alphas (list): Lista de valores alpha a probar.\n",
        "        X_train, y_train: Datos de entrenamiento.\n",
        "        X_test, y_test: Datos de prueba.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (mejor_alpha, mejor_r2, mejor_mse, mejor_modelo)\n",
        "    \"\"\"\n",
        "    best_r2 = -np.inf  # Inicializar con un valor muy bajo para R^2\n",
        "    best_mse = np.inf   # Inicializar con un valor muy alto para MSE\n",
        "    best_alpha = None\n",
        "    best_model = None\n",
        "\n",
        "    print(f\"\\n--- Buscando el mejor alpha para {model_type.capitalize()} ---\")\n",
        "    print(f\"Alphas a probar: {alphas}\")\n",
        "\n",
        "    for alpha in alphas:\n",
        "        if model_type == 'ridge':\n",
        "            model = Ridge(alpha=alpha, random_state=42)\n",
        "        elif model_type == 'lasso':\n",
        "            # Aumentar max_iter puede ser necesario para que Lasso converja\n",
        "            model = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
        "        else:\n",
        "            raise ValueError(\"model_type debe ser 'ridge' o 'lasso'\")\n",
        "\n",
        "        # Entrenar el modelo\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predecir en el conjunto de prueba\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Evaluar el modelo\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"  Alpha={alpha:.4f} -> R² = {r2:.4f}, MSE = {mse:.4f}\")\n",
        "\n",
        "        # Actualizar si encontramos un mejor modelo (basado en R²)\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_mse = mse\n",
        "            best_alpha = alpha\n",
        "            best_model = model\n",
        "\n",
        "    print(f\"\\nMejor alpha para {model_type.capitalize()}: {best_alpha}\")\n",
        "    print(f\"  Mejor R²: {best_r2:.4f}\")\n",
        "    print(f\"  Mejor MSE: {best_mse:.4f}\")\n",
        "\n",
        "    return best_alpha, best_r2, best_mse, best_model\n",
        "\n",
        "# --- 6. Definir Rangos de Alpha y Ejecutar la Búsqueda ---\n",
        "\n",
        "# Rangos de alpha especificados\n",
        "ridge_alphas = list(range(2, 11)) # Enteros del 2 al 10\n",
        "lasso_alphas = [0.1, 1, 10]\n",
        "\n",
        "# Encontrar el mejor alpha para Ridge\n",
        "best_alpha_ridge, best_r2_ridge, best_mse_ridge, best_ridge_model = find_best_alpha(\n",
        "    'ridge', ridge_alphas, X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "\n",
        "# Encontrar el mejor alpha para Lasso\n",
        "best_alpha_lasso, best_r2_lasso, best_mse_lasso, best_lasso_model = find_best_alpha(\n",
        "    'lasso', lasso_alphas, X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "\n",
        "# --- 7. Comparación y Resultados Finales ---\n",
        "\n",
        "print(\"\\n--- Resumen de los Mejores Modelos Regularizados ---\")\n",
        "print(f\"Mejor Ridge: Alpha={best_alpha_ridge}, R²={best_r2_ridge:.4f}, MSE={best_mse_ridge:.4f}\")\n",
        "print(f\"Mejor Lasso: Alpha={best_alpha_lasso}, R²={best_r2_lasso:.4f}, MSE={best_mse_lasso:.4f}\")\n",
        "\n",
        "# (Opcional) Entrenar un modelo de Regresión Lineal simple como baseline\n",
        "print(\"\\n--- Evaluando Regresión Lineal Simple (Baseline) ---\")\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "print(f\"Regresión Lineal Simple: R²={r2_lr:.4f}, MSE={mse_lr:.4f}\")\n",
        "\n",
        "# (Opcional) Ver los coeficientes del mejor modelo Lasso\n",
        "if best_lasso_model:\n",
        "    print(f\"\\n--- Coeficientes del Mejor Modelo Lasso (Alpha={best_alpha_lasso}) ---\")\n",
        "    lasso_coeffs = pd.DataFrame({\n",
        "        'Característica': X.columns,\n",
        "        'Coeficiente': best_lasso_model.coef_\n",
        "    })\n",
        "    # Mostrar coeficientes no nulos (Lasso realiza selección de características)\n",
        "    print(lasso_coeffs[lasso_coeffs['Coeficiente'] != 0].sort_values(by='Coeficiente', ascending=False))\n",
        "\n",
        "# (Opcional) Ver los coeficientes del mejor modelo Ridge\n",
        "if best_ridge_model:\n",
        "     print(f\"\\n--- Coeficientes del Mejor Modelo Ridge (Alpha={best_alpha_ridge}) ---\")\n",
        "     ridge_coeffs = pd.DataFrame({\n",
        "        'Característica': X.columns,\n",
        "        'Coeficiente': best_ridge_model.coef_\n",
        "     })\n",
        "     print(ridge_coeffs.sort_values(by='Coeficiente', ascending=False))\n",
        "\n",
        "print(\"\\n¡Análisis completado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xQ64NAueMbY",
        "outputId": "5ed294a5-6b51-46fd-be09-efdd9ea4837c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos cargados exitosamente desde '/content/drive/MyDrive/PROYECTO_FINAL/dataset/playlist_2010to2022.csv'\n",
            "Número de filas: 2300, Número de columnas: 23\n",
            "\n",
            "Valores faltantes antes de limpiar (en subset inicial):\n",
            "track_popularity     0\n",
            "year                 0\n",
            "artist_popularity    0\n",
            "danceability         1\n",
            "energy               1\n",
            "key                  1\n",
            "loudness             1\n",
            "mode                 1\n",
            "speechiness          1\n",
            "acousticness         1\n",
            "instrumentalness     1\n",
            "liveness             1\n",
            "valence              1\n",
            "tempo                1\n",
            "duration_ms          1\n",
            "time_signature       1\n",
            "dtype: int64\n",
            "\n",
            "Forma del dataset después de limpiar NaNs numéricos: (2299, 17)\n",
            "\n",
            "--- Iniciando Ingeniería de Características ---\n",
            "\n",
            "Procesando 'artist_genres'...\n",
            "Top 20 géneros encontrados: ['pop', 'dance pop', 'rap', 'pop rap', 'hip hop', 'r&b', 'urban contemporary', 'trap', 'southern hip hop', 'modern rock', 'rock', 'canadian pop', 'edm', 'hip pop', 'pop dance', 'atl hip hop', 'uk pop', 'neo mellow', 'gangster rap', 'post-grunge']\n",
            "Se añadieron 20 columnas de género (One-Hot Encoded).\n",
            "\n",
            "Aplicando transformaciones no lineales...\n",
            " - Columna 'duration_ms_log' creada.\n",
            " - Columna 'speechiness_log' creada.\n",
            " - Columna 'liveness_log' creada.\n",
            " - Columna 'instrumentalness_log' creada.\n",
            " - Columna 'artist_popularity_sq' creada.\n",
            " - Columna 'danceability_sq' creada.\n",
            " - Columna 'loudness_sq' creada.\n",
            " - Columna 'energy_sq' creada.\n",
            " - Columna 'valence_sq' creada.\n",
            "\n",
            "Creando características de interacción...\n",
            " - Interacción 'danceability_x_energy' creada.\n",
            " - Interacción 'valence_x_energy' creada.\n",
            "\n",
            "Aplicando binning a 'artist_popularity'...\n",
            " - Columna 'artist_popularity_binned' creada con 5 bins (ordinal).\n",
            " - Columnas One-Hot creadas para 'artist_popularity_binned'.\n",
            " - Columna 'artist_popularity' original eliminada.\n",
            "\n",
            "--- Ingeniería de Características Completada ---\n",
            "\n",
            "Advertencia: Se encontraron columnas no numéricas después de la ingeniería: ['artist_popularity_binned_0.0', 'artist_popularity_binned_1.0', 'artist_popularity_binned_2.0', 'artist_popularity_binned_3.0', 'artist_popularity_binned_4.0']\n",
            "Intentando convertir a numérico o eliminándolas...\n",
            "\n",
            "Forma final de X (características ingenierizadas): (2299, 50)\n",
            "\n",
            "Columnas finales en X_engineered:\n",
            "['year', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature', 'genre_pop', 'genre_dance_pop', 'genre_rap', 'genre_pop_rap', 'genre_hip_hop', 'genre_r&b', 'genre_urban_contemporary', 'genre_trap', 'genre_southern_hip_hop', 'genre_modern_rock', 'genre_rock', 'genre_canadian_pop', 'genre_edm', 'genre_hip_pop', 'genre_pop_dance', 'genre_atl_hip_hop', 'genre_uk_pop', 'genre_neo_mellow', 'genre_gangster_rap', 'genre_post_grunge', 'duration_ms_log', 'speechiness_log', 'liveness_log', 'instrumentalness_log', 'artist_popularity_sq', 'danceability_sq', 'loudness_sq', 'energy_sq', 'valence_sq', 'danceability_x_energy', 'valence_x_energy', 'artist_popularity_binned_0.0', 'artist_popularity_binned_1.0', 'artist_popularity_binned_2.0', 'artist_popularity_binned_3.0', 'artist_popularity_binned_4.0']\n",
            "\n",
            "Primeras filas de X_engineered:\n",
            "   year  danceability  energy   key  loudness  mode  speechiness  \\\n",
            "0  2000         0.429   0.661  11.0    -7.227   1.0       0.0281   \n",
            "1  2000         0.434   0.897   0.0    -4.918   1.0       0.0488   \n",
            "2  2000         0.529   0.496   7.0    -9.007   1.0       0.0290   \n",
            "3  2000         0.556   0.864   3.0    -5.870   0.0       0.0584   \n",
            "4  2000         0.610   0.926   8.0    -4.843   0.0       0.0479   \n",
            "\n",
            "   acousticness  instrumentalness  liveness  ...  loudness_sq  energy_sq  \\\n",
            "0       0.00239          0.000121    0.2340  ...    52.229529   0.436921   \n",
            "1       0.01030          0.000000    0.6120  ...    24.186724   0.804609   \n",
            "2       0.17300          0.000000    0.2510  ...    81.126049   0.246016   \n",
            "3       0.00958          0.000000    0.2090  ...    34.456900   0.746496   \n",
            "4       0.03100          0.001200    0.0821  ...    23.454649   0.857476   \n",
            "\n",
            "   valence_sq  danceability_x_energy  valence_x_energy  \\\n",
            "0    0.081225               0.283569          0.188385   \n",
            "1    0.467856               0.389298          0.613548   \n",
            "2    0.077284               0.262384          0.137888   \n",
            "3    0.160000               0.480384          0.345600   \n",
            "4    0.741321               0.564860          0.797286   \n",
            "\n",
            "   artist_popularity_binned_0.0  artist_popularity_binned_1.0  \\\n",
            "0                         False                         False   \n",
            "1                         False                         False   \n",
            "2                          True                         False   \n",
            "3                         False                         False   \n",
            "4                         False                          True   \n",
            "\n",
            "   artist_popularity_binned_2.0  artist_popularity_binned_3.0  \\\n",
            "0                         False                         False   \n",
            "1                          True                         False   \n",
            "2                         False                         False   \n",
            "3                         False                          True   \n",
            "4                         False                         False   \n",
            "\n",
            "   artist_popularity_binned_4.0  \n",
            "0                          True  \n",
            "1                         False  \n",
            "2                         False  \n",
            "3                         False  \n",
            "4                         False  \n",
            "\n",
            "[5 rows x 50 columns]\n",
            "\n",
            "--- Buscando el mejor alpha para Ridge ---\n",
            "Alphas a probar: [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "  Alpha=2.0000 -> R² = 0.0954, MSE = 146.2492\n",
            "  Alpha=3.0000 -> R² = 0.0957, MSE = 146.1997\n",
            "  Alpha=4.0000 -> R² = 0.0960, MSE = 146.1530\n",
            "  Alpha=5.0000 -> R² = 0.0963, MSE = 146.1091\n",
            "  Alpha=6.0000 -> R² = 0.0965, MSE = 146.0682\n",
            "  Alpha=7.0000 -> R² = 0.0967, MSE = 146.0300\n",
            "  Alpha=8.0000 -> R² = 0.0970, MSE = 145.9943\n",
            "  Alpha=9.0000 -> R² = 0.0972, MSE = 145.9609\n",
            "  Alpha=10.0000 -> R² = 0.0974, MSE = 145.9298\n",
            "\n",
            "Mejor alpha para Ridge: 10\n",
            "  Mejor R²: 0.0974\n",
            "  Mejor MSE: 145.9298\n",
            "\n",
            "--- Buscando el mejor alpha para Lasso ---\n",
            "Alphas a probar: [0.1, 1, 10]\n",
            "  Alpha=0.1000 -> R² = 0.1058, MSE = 144.5707\n",
            "  Alpha=1.0000 -> R² = 0.0783, MSE = 149.0179\n",
            "  Alpha=10.0000 -> R² = -0.0000, MSE = 161.6703\n",
            "\n",
            "Mejor alpha para Lasso: 0.1\n",
            "  Mejor R²: 0.1058\n",
            "  Mejor MSE: 144.5707\n",
            "\n",
            "--- Resumen de los Mejores Modelos Regularizados ---\n",
            "Mejor Ridge: Alpha=10, R²=0.0974, MSE=145.9298\n",
            "Mejor Lasso: Alpha=0.1, R²=0.1058, MSE=144.5707\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast # Para convertir string de listas a listas reales\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, KBinsDiscretizer # Para binning y OHE\n",
        "import warnings\n",
        "\n",
        "# Ignorar advertencias (opcional)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Carga y Limpieza Inicial (Asegúrate de que esto se ejecute primero) ---\n",
        "# (Copio las partes relevantes del código anterior para que sea autocontenido)\n",
        "\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "df = data.copy()\n",
        "print(f\"Datos cargados exitosamente desde '{file_path}'\")\n",
        "print(f\"Número de filas: {df.shape[0]}, Número de columnas: {df.shape[1]}\")\n",
        "# Seleccionar columnas relevantes iniciales (incluyendo genres ahora)\n",
        "initial_features = [\n",
        "    'track_popularity', 'year', 'artist_popularity', 'danceability',\n",
        "    'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
        "    'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms',\n",
        "    'time_signature', 'artist_genres' # Incluimos géneros\n",
        "]\n",
        "\n",
        "existing_features = [col for col in initial_features if col in df.columns]\n",
        "df_subset = df[existing_features].copy()\n",
        "\n",
        "# Convertir columnas numéricas (excluyendo genres por ahora)\n",
        "numeric_cols_to_check = [col for col in existing_features if col != 'artist_genres' and col != 'track_popularity']\n",
        "for col in numeric_cols_to_check:\n",
        "    df_subset[col] = pd.to_numeric(df_subset[col], errors='coerce')\n",
        "\n",
        "# Manejar valores faltantes en columnas numéricas y target (eliminar filas)\n",
        "cols_to_check_na = [col for col in existing_features if col != 'artist_genres']\n",
        "print(f\"\\nValores faltantes antes de limpiar (en subset inicial):\\n{df_subset[cols_to_check_na].isnull().sum()}\")\n",
        "df_subset.dropna(subset=cols_to_check_na, inplace=True)\n",
        "print(f\"\\nForma del dataset después de limpiar NaNs numéricos: {df_subset.shape}\")\n",
        "\n",
        "# Asegurarse de que 'artist_genres' sea string y manejar NaNs (reemplazar con '[]')\n",
        "if 'artist_genres' in df_subset.columns:\n",
        "    df_subset['artist_genres'] = df_subset['artist_genres'].fillna('[]').astype(str)\n",
        "else:\n",
        "    print(\"Advertencia: La columna 'artist_genres' no se encontró.\")\n",
        "\n",
        "if df_subset.empty:\n",
        "    print(\"Error: El dataset quedó vacío después de la limpieza inicial.\")\n",
        "    exit()\n",
        "\n",
        "# Separar X e y TEMPORALMENTE para aplicar ingeniería solo a X\n",
        "X_temp = df_subset.drop('track_popularity', axis=1)\n",
        "y = df_subset['track_popularity'] # y ya está limpio y listo\n",
        "\n",
        "print(\"\\n--- Iniciando Ingeniería de Características ---\")\n",
        "\n",
        "# Crear una copia para no modificar X_temp directamente en cada paso\n",
        "X_engineered = X_temp.copy()\n",
        "\n",
        "# --- 1. Procesamiento de Géneros (artist_genres) ---\n",
        "if 'artist_genres' in X_engineered.columns:\n",
        "    print(\"\\nProcesando 'artist_genres'...\")\n",
        "    # Función segura para convertir string de lista a lista\n",
        "    def parse_genre_list(genres_str):\n",
        "        try:\n",
        "            # Evaluar la cadena para obtener la lista\n",
        "            genres = ast.literal_eval(genres_str)\n",
        "            # Asegurarse de que sea una lista (puede ser None o algo más si la celda estaba vacía o mal formateada)\n",
        "            if isinstance(genres, list):\n",
        "                return genres\n",
        "            else:\n",
        "                return [] # Devolver lista vacía si no es una lista\n",
        "        except (ValueError, SyntaxError):\n",
        "            # Si falla la evaluación (ej. no es formato lista), devolver lista vacía\n",
        "            return []\n",
        "\n",
        "    # Aplicar la función de parseo\n",
        "    X_engineered['genre_list'] = X_engineered['artist_genres'].apply(parse_genre_list)\n",
        "\n",
        "    # Contar la frecuencia de cada género individual\n",
        "    all_genres = [genre for sublist in X_engineered['genre_list'] for genre in sublist]\n",
        "    genre_counts = Counter(all_genres)\n",
        "\n",
        "    # Definir cuántos géneros top queremos codificar (ajusta N según necesites)\n",
        "    N_TOP_GENRES = 20\n",
        "    top_genres = [genre for genre, count in genre_counts.most_common(N_TOP_GENRES)]\n",
        "    print(f\"Top {N_TOP_GENRES} géneros encontrados: {top_genres}\")\n",
        "\n",
        "    # Crear columnas One-Hot para los géneros top\n",
        "    for genre in top_genres:\n",
        "        # Nueva columna se llamará 'genre_...'\n",
        "        col_name = f'genre_{genre.replace(\" \", \"_\").replace(\"-\", \"_\")}' # Limpiar nombre\n",
        "        X_engineered[col_name] = X_engineered['genre_list'].apply(lambda x: 1 if genre in x else 0)\n",
        "\n",
        "    # Eliminar columnas intermedias y originales de género\n",
        "    X_engineered.drop(['artist_genres', 'genre_list'], axis=1, inplace=True)\n",
        "    print(f\"Se añadieron {N_TOP_GENRES} columnas de género (One-Hot Encoded).\")\n",
        "else:\n",
        "    print(\"\\n'artist_genres' no presente, saltando procesamiento de géneros.\")\n",
        "\n",
        "\n",
        "# --- 2. Transformaciones No Lineales ---\n",
        "print(\"\\nAplicando transformaciones no lineales...\")\n",
        "# Logaritmo (usar log1p para manejar ceros) a características potencialmente sesgadas\n",
        "# Asegúrate de que estas columnas existan antes de intentar transformarlas\n",
        "log_features = ['duration_ms', 'speechiness', 'liveness', 'instrumentalness']\n",
        "for col in log_features:\n",
        "    if col in X_engineered.columns:\n",
        "        # Verificar si hay valores negativos antes de aplicar log\n",
        "        if (X_engineered[col] < 0).any():\n",
        "            print(f\"Advertencia: La columna '{col}' contiene valores negativos. No se aplicará log.\")\n",
        "        else:\n",
        "            X_engineered[f'{col}_log'] = np.log1p(X_engineered[col])\n",
        "            print(f\" - Columna '{col}_log' creada.\")\n",
        "            # Decisión: ¿Eliminar la original? Por ahora la dejamos, Lasso podría eliminarla si no es útil.\n",
        "            # X_engineered.drop(col, axis=1, inplace=True)\n",
        "\n",
        "\n",
        "# Términos cuadráticos para algunas características clave\n",
        "poly_features = ['artist_popularity', 'danceability', 'loudness', 'energy', 'valence']\n",
        "for col in poly_features:\n",
        "     if col in X_engineered.columns:\n",
        "        X_engineered[f'{col}_sq'] = X_engineered[col]**2\n",
        "        print(f\" - Columna '{col}_sq' creada.\")\n",
        "        # Nuevamente, dejamos la original por ahora\n",
        "\n",
        "\n",
        "# --- 3. Creación de Interacciones ---\n",
        "print(\"\\nCreando características de interacción...\")\n",
        "interactions = [\n",
        "    ('danceability', 'energy'),\n",
        "    ('valence', 'energy'),\n",
        "    #('artist_popularity', 'acousticness') # Comentado, puedes añadir las que creas relevantes\n",
        "]\n",
        "\n",
        "for col1, col2 in interactions:\n",
        "    if col1 in X_engineered.columns and col2 in X_engineered.columns:\n",
        "        X_engineered[f'{col1}_x_{col2}'] = X_engineered[col1] * X_engineered[col2]\n",
        "        print(f\" - Interacción '{col1}_x_{col2}' creada.\")\n",
        "\n",
        "# Podrías crear interacciones con los géneros OHE si crees que es relevante\n",
        "# Ejemplo: interactuar danceability con el género pop\n",
        "# if 'genre_pop' in X_engineered.columns and 'danceability' in X_engineered.columns:\n",
        "#    X_engineered['danceability_x_pop'] = X_engineered['danceability'] * X_engineered['genre_pop']\n",
        "#    print(\" - Interacción 'danceability_x_pop' creada.\")\n",
        "\n",
        "\n",
        "# --- 4. Binning (Agrupación) ---\n",
        "print(\"\\nAplicando binning a 'artist_popularity'...\")\n",
        "if 'artist_popularity' in X_engineered.columns:\n",
        "    N_BINS = 5 # Número de contenedores (ej. 5 para quintiles)\n",
        "    bin_col_name = 'artist_popularity_binned'\n",
        "\n",
        "    # Usaremos KBinsDiscretizer para manejar mejor los bordes y codificación\n",
        "    # Estrategia 'quantile' para tener aprox. el mismo número de muestras por bin\n",
        "    try:\n",
        "        discretizer = KBinsDiscretizer(n_bins=N_BINS, encode='ordinal', strategy='quantile')\n",
        "        # Usar reshape(-1, 1) ya que espera una entrada 2D\n",
        "        X_engineered[bin_col_name] = discretizer.fit_transform(X_engineered[['artist_popularity']])\n",
        "        print(f\" - Columna '{bin_col_name}' creada con {N_BINS} bins (ordinal).\")\n",
        "\n",
        "        # Ahora, aplicar One-Hot Encoding a los bins, ya que son categóricos\n",
        "        ohe_binned = pd.get_dummies(X_engineered[bin_col_name], prefix=bin_col_name)\n",
        "        # Concatenar las nuevas columnas OHE\n",
        "        X_engineered = pd.concat([X_engineered, ohe_binned], axis=1)\n",
        "        # Eliminar la columna ordinal intermedia\n",
        "        X_engineered.drop(bin_col_name, axis=1, inplace=True)\n",
        "        print(f\" - Columnas One-Hot creadas para '{bin_col_name}'.\")\n",
        "\n",
        "        # Decisión: ¿Eliminar la 'artist_popularity' original ahora que está binarizada?\n",
        "        # Es común hacerlo para evitar redundancia con los bins.\n",
        "        X_engineered.drop('artist_popularity', axis=1, inplace=True)\n",
        "        print(f\" - Columna 'artist_popularity' original eliminada.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error durante el binning de 'artist_popularity': {e}. Saltando este paso.\")\n",
        "\n",
        "else:\n",
        "    print(\"'artist_popularity' no encontrada, saltando binning.\")\n",
        "\n",
        "\n",
        "# --- Finalización ---\n",
        "print(\"\\n--- Ingeniería de Características Completada ---\")\n",
        "\n",
        "# Eliminar columnas que puedan contener NaNs introducidos (si los hubiera)\n",
        "# X_engineered.dropna(inplace=True) # Cuidado: esto podría eliminar muchas filas si alguna transformación falló\n",
        "\n",
        "# Asegurarse de que todas las columnas sean numéricas\n",
        "# (OHE y transformaciones deberían serlo, pero es bueno verificar)\n",
        "non_numeric_cols = X_engineered.select_dtypes(exclude=np.number).columns\n",
        "if len(non_numeric_cols) > 0:\n",
        "    print(f\"\\nAdvertencia: Se encontraron columnas no numéricas después de la ingeniería: {list(non_numeric_cols)}\")\n",
        "    print(\"Intentando convertir a numérico o eliminándolas...\")\n",
        "    # Intenta convertir forzadamente, si falla, considera eliminarlas o revisa el paso que las creó\n",
        "    for col in non_numeric_cols:\n",
        "        X_engineered[col] = pd.to_numeric(X_engineered[col], errors='coerce')\n",
        "    X_engineered.dropna(axis=1, inplace=True) # Elimina columnas que no se pudieron convertir\n",
        "\n",
        "# Verificar si todavía quedan NaNs en las filas\n",
        "if X_engineered.isnull().sum().sum() > 0:\n",
        "     print(\"\\nAdvertencia: Aún quedan valores NaN después de la ingeniería. Considera imputar o eliminar filas.\")\n",
        "     # Opción: eliminar filas con NaN restantes\n",
        "     # original_rows = X_engineered.shape[0]\n",
        "     # y = y[X_engineered.index] # Asegurar alineación antes de dropear filas en X\n",
        "     # X_engineered.dropna(axis=0, inplace=True)\n",
        "     # print(f\"Se eliminaron {original_rows - X_engineered.shape[0]} filas con NaN.\")\n",
        "\n",
        "\n",
        "print(f\"\\nForma final de X (características ingenierizadas): {X_engineered.shape}\")\n",
        "print(\"\\nColumnas finales en X_engineered:\")\n",
        "print(X_engineered.columns.tolist())\n",
        "print(\"\\nPrimeras filas de X_engineered:\")\n",
        "print(X_engineered.head())\n",
        "\n",
        "# --- ¡Listo para Modelar! ---\n",
        "# Ahora puedes usar X_engineered y 'y' para dividir, escalar y entrenar tus modelos\n",
        "# como en el código anterior.\n",
        "\n",
        "# Ejemplo de cómo continuar:\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# 1. División Train/Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Escalado (IMPORTANTE: Escalar DESPUÉS de dividir y DESPUÉS de la ingeniería)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Búsqueda de Alpha y Entrenamiento (usando la función find_best_alpha definida antes)\n",
        "best_alpha_ridge, best_r2_ridge, best_mse_ridge, best_ridge_model = find_best_alpha('ridge', ridge_alphas, X_train_scaled, y_train, X_test_scaled, y_test )\n",
        "best_alpha_lasso, best_r2_lasso, best_mse_lasso, best_lasso_model = find_best_alpha('lasso', lasso_alphas, X_train_scaled, y_train, X_test_scaled, y_test)\n",
        "\n",
        "# 4. Evaluación y Comparación\n",
        "# ... (código de evaluación anterior) ...\n",
        "print(\"\\n--- Resumen de los Mejores Modelos Regularizados ---\")\n",
        "print(f\"Mejor Ridge: Alpha={best_alpha_ridge}, R²={best_r2_ridge:.4f}, MSE={best_mse_ridge:.4f}\")\n",
        "print(f\"Mejor Lasso: Alpha={best_alpha_lasso}, R²={best_r2_lasso:.4f}, MSE={best_mse_lasso:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY5CKedjBWhr",
        "outputId": "1a53888f-b02e-42ab-e4d5-0b014aeebbcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: No se encontró el archivo en la ruta: tu_dataset.csv\n",
            "Por favor, asegúrate de que el archivo exista y la ruta sea correcta.\n",
            "\n",
            "Valores faltantes antes de limpiar:\n",
            "year                 0\n",
            "track_popularity     0\n",
            "artist_popularity    0\n",
            "danceability         1\n",
            "energy               1\n",
            "key                  1\n",
            "loudness             1\n",
            "mode                 1\n",
            "speechiness          1\n",
            "acousticness         1\n",
            "instrumentalness     1\n",
            "liveness             1\n",
            "valence              1\n",
            "tempo                1\n",
            "duration_ms          1\n",
            "time_signature       1\n",
            "dtype: int64\n",
            "\n",
            "Forma del dataset después de limpiar NaNs: (2299, 16)\n",
            "\n",
            "Características seleccionadas (X):\n",
            "['year', 'artist_popularity', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature']\n",
            "\n",
            "Variable objetivo (y): track_popularity\n",
            "\n",
            "Tamaño del conjunto de entrenamiento: 1839 muestras\n",
            "Tamaño del conjunto de prueba: 460 muestras\n",
            "\n",
            "Datos normalizados (primeras 5 filas de entrenamiento):\n",
            "[[ 0.00295987  1.1625506  -0.13218844 -2.46835388  1.04241393 -2.19670445\n",
            "   0.82988266 -0.7659826   2.29920224 -0.62261332  0.14758783 -0.56531144\n",
            "  -1.28632296  0.89826829  0.        ]\n",
            " [-1.20663887 -0.722822   -0.10353894 -0.55668027  1.04241393 -0.43306323\n",
            "   0.82988266  2.11873622  0.50239221 -0.62261332 -0.80616418  1.21797096\n",
            "   2.54751341 -0.01749909  0.        ]\n",
            " [-0.29943982  0.99860515 -2.38833609 -0.6304426  -0.33533454 -0.45518116\n",
            "   0.82988266 -0.81183047 -0.37651139 -0.62261332 -0.57294052 -0.90960853\n",
            "   1.43738138  0.30445704  0.        ]\n",
            " [ 1.2125586  -0.39493111  1.18568829  0.50672659  1.59351332  0.80501395\n",
            "  -1.20498963 -0.53378529  1.0502183  -0.62261332 -0.20319571  0.9972677\n",
            "   0.2138661   0.36123994  0.        ]\n",
            " [-1.35783871  0.58874155 -0.3040854   1.52095856  1.04241393  1.21366895\n",
            "   0.82988266 -0.63435482 -0.76653975 -0.62261332  0.02433956  1.50047115\n",
            "  -1.05508126  1.1811712   0.        ]]\n",
            "\n",
            "--- Buscando el mejor alpha para Ridge ---\n",
            "Alphas a probar: [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "  Alpha=2.0000 -> R² = 0.0840, MSE = 148.0863\n",
            "  Alpha=3.0000 -> R² = 0.0840, MSE = 148.0835\n",
            "  Alpha=4.0000 -> R² = 0.0841, MSE = 148.0807\n",
            "  Alpha=5.0000 -> R² = 0.0841, MSE = 148.0780\n",
            "  Alpha=6.0000 -> R² = 0.0841, MSE = 148.0752\n",
            "  Alpha=7.0000 -> R² = 0.0841, MSE = 148.0725\n",
            "  Alpha=8.0000 -> R² = 0.0841, MSE = 148.0698\n",
            "  Alpha=9.0000 -> R² = 0.0841, MSE = 148.0671\n",
            "  Alpha=10.0000 -> R² = 0.0842, MSE = 148.0644\n",
            "\n",
            "Mejor alpha para Ridge: 10\n",
            "  Mejor R²: 0.0842\n",
            "  Mejor MSE: 148.0644\n",
            "\n",
            "--- Buscando el mejor alpha para Lasso ---\n",
            "Alphas a probar: [0.1, 1, 10]\n",
            "  Alpha=0.1000 -> R² = 0.0865, MSE = 147.6865\n",
            "  Alpha=1.0000 -> R² = 0.0803, MSE = 148.6953\n",
            "  Alpha=10.0000 -> R² = -0.0000, MSE = 161.6703\n",
            "\n",
            "Mejor alpha para Lasso: 0.1\n",
            "  Mejor R²: 0.0865\n",
            "  Mejor MSE: 147.6865\n",
            "\n",
            "--- Resumen de los Mejores Modelos Regularizados ---\n",
            "Mejor Ridge: Alpha=10, R²=0.0842, MSE=148.0644\n",
            "Mejor Lasso: Alpha=0.1, R²=0.0865, MSE=147.6865\n",
            "\n",
            "--- Evaluando Regresión Lineal Simple (Baseline) ---\n",
            "Regresión Lineal Simple: R²=0.0840, MSE=148.0919\n",
            "\n",
            "--- Coeficientes del Mejor Modelo Lasso (Alpha=0.1) ---\n",
            "       Característica  Coeficiente\n",
            "1   artist_popularity     3.675002\n",
            "0                year     0.869671\n",
            "8        acousticness     0.222130\n",
            "9    instrumentalness     0.073979\n",
            "2        danceability     0.037434\n",
            "6                mode     0.035001\n",
            "11            valence    -0.013928\n",
            "3              energy    -0.064466\n",
            "4                 key    -0.326335\n",
            "10           liveness    -0.364811\n",
            "7         speechiness    -0.536058\n",
            "\n",
            "--- Coeficientes del Mejor Modelo Ridge (Alpha=10) ---\n",
            "       Característica  Coeficiente\n",
            "1   artist_popularity     3.727778\n",
            "0                year     0.854786\n",
            "8        acousticness     0.313893\n",
            "2        danceability     0.236666\n",
            "5            loudness     0.216206\n",
            "9    instrumentalness     0.186462\n",
            "12              tempo     0.134094\n",
            "6                mode     0.121640\n",
            "14     time_signature     0.000000\n",
            "13        duration_ms    -0.052029\n",
            "11            valence    -0.164025\n",
            "3              energy    -0.210265\n",
            "4                 key    -0.413792\n",
            "10           liveness    -0.417711\n",
            "7         speechiness    -0.629039\n",
            "\n",
            "¡Análisis completado!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler # Para escalar características\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. Cargar los Datos ---\n",
        "# !!! Reemplaza 'tu_archivo.csv' con el nombre real de tu archivo de datos !!!\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "df\n",
        "print(f\"Datos cargados exitosamente desde '{file_path}'\")\n",
        "print(f\"Número de filas: {df.shape[0]}, Número de columnas: {df.shape[1]}\")\n",
        "# print(\"\\nInformación del DataFrame:\")\n",
        "# df.info()\n",
        "\n",
        "\n",
        "# --- 2. Selección de Características y Preprocesamiento ---\n",
        "\n",
        "# Seleccionar características numéricas relevantes para la predicción\n",
        "# Excluimos IDs, nombres, URLs y géneros (que requerirían un preprocesamiento más complejo)\n",
        "# También excluimos el target 'track_popularity' de las features X\n",
        "numeric_features = [\n",
        "    'year', 'track_popularity', 'artist_popularity', 'danceability',\n",
        "    'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
        "    'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms',\n",
        "    'time_signature'\n",
        "]\n",
        "\n",
        "# Asegurarnos de que solo trabajamos con columnas que existen y son numéricas\n",
        "existing_features = [col for col in numeric_features if col in df.columns]\n",
        "df_subset = df[existing_features].copy()\n",
        "\n",
        "# Convertir columnas a numéricas si es necesario (forzando errores a NaN)\n",
        "for col in existing_features:\n",
        "    df_subset[col] = pd.to_numeric(df_subset[col], errors='coerce')\n",
        "\n",
        "# Manejar valores faltantes (una estrategia simple: eliminar filas con NaN en las columnas seleccionadas)\n",
        "print(f\"\\nValores faltantes antes de limpiar:\\n{df_subset.isnull().sum()}\")\n",
        "df_subset.dropna(inplace=True)\n",
        "print(f\"\\nForma del dataset después de limpiar NaNs: {df_subset.shape}\")\n",
        "\n",
        "if df_subset.empty:\n",
        "    print(\"Error: El dataset quedó vacío después de eliminar filas con valores faltantes.\")\n",
        "    exit()\n",
        "\n",
        "# Separar características (X) y variable objetivo (y)\n",
        "X = df_subset.drop('track_popularity', axis=1)\n",
        "y = df_subset['track_popularity']\n",
        "\n",
        "# Verificar que aún tenemos datos\n",
        "if X.empty or y.empty:\n",
        "    print(\"Error: No quedan datos suficientes después de la selección de características y limpieza.\")\n",
        "    exit()\n",
        "\n",
        "print(\"\\nCaracterísticas seleccionadas (X):\")\n",
        "print(X.columns.tolist())\n",
        "print(\"\\nVariable objetivo (y): track_popularity\")\n",
        "\n",
        "# --- 3. División en Conjuntos de Entrenamiento y Prueba ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"\\nTamaño del conjunto de entrenamiento: {X_train.shape[0]} muestras\")\n",
        "print(f\"Tamaño del conjunto de prueba: {X_test.shape[0]} muestras\")\n",
        "\n",
        "# --- 4. Normalización (Escalado) de Datos ---\n",
        "# Es crucial escalar DESPUÉS de dividir para evitar fuga de datos del test set al scaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nDatos normalizados (primeras 5 filas de entrenamiento):\")\n",
        "print(X_train_scaled[:5])\n",
        "\n",
        "# --- 5. Función para Encontrar el Mejor Alpha ---\n",
        "def find_best_alpha(model_type, alphas, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Entrena modelos Ridge o Lasso con diferentes alphas y encuentra el mejor.\n",
        "\n",
        "    Args:\n",
        "        model_type (str): 'ridge' o 'lasso'.\n",
        "        alphas (list): Lista de valores alpha a probar.\n",
        "        X_train, y_train: Datos de entrenamiento.\n",
        "        X_test, y_test: Datos de prueba.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (mejor_alpha, mejor_r2, mejor_mse, mejor_modelo)\n",
        "    \"\"\"\n",
        "    best_r2 = -np.inf  # Inicializar con un valor muy bajo para R^2\n",
        "    best_mse = np.inf   # Inicializar con un valor muy alto para MSE\n",
        "    best_alpha = None\n",
        "    best_model = None\n",
        "\n",
        "    print(f\"\\n--- Buscando el mejor alpha para {model_type.capitalize()} ---\")\n",
        "    print(f\"Alphas a probar: {alphas}\")\n",
        "\n",
        "    for alpha in alphas:\n",
        "        if model_type == 'ridge':\n",
        "            model = Ridge(alpha=alpha, random_state=42)\n",
        "        elif model_type == 'lasso':\n",
        "            # Aumentar max_iter puede ser necesario para que Lasso converja\n",
        "            model = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
        "        else:\n",
        "            raise ValueError(\"model_type debe ser 'ridge' o 'lasso'\")\n",
        "\n",
        "        # Entrenar el modelo\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predecir en el conjunto de prueba\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Evaluar el modelo\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"  Alpha={alpha:.4f} -> R² = {r2:.4f}, MSE = {mse:.4f}\")\n",
        "\n",
        "        # Actualizar si encontramos un mejor modelo (basado en R²)\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_mse = mse\n",
        "            best_alpha = alpha\n",
        "            best_model = model\n",
        "\n",
        "    print(f\"\\nMejor alpha para {model_type.capitalize()}: {best_alpha}\")\n",
        "    print(f\"  Mejor R²: {best_r2:.4f}\")\n",
        "    print(f\"  Mejor MSE: {best_mse:.4f}\")\n",
        "\n",
        "    return best_alpha, best_r2, best_mse, best_model\n",
        "\n",
        "# --- 6. Definir Rangos de Alpha y Ejecutar la Búsqueda ---\n",
        "\n",
        "# Rangos de alpha especificados\n",
        "ridge_alphas = list(range(2, 11)) # Enteros del 2 al 10\n",
        "lasso_alphas = [0.1, 1, 10]\n",
        "\n",
        "# Encontrar el mejor alpha para Ridge\n",
        "best_alpha_ridge, best_r2_ridge, best_mse_ridge, best_ridge_model = find_best_alpha(\n",
        "    'ridge', ridge_alphas, X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "\n",
        "# Encontrar el mejor alpha para Lasso\n",
        "best_alpha_lasso, best_r2_lasso, best_mse_lasso, best_lasso_model = find_best_alpha(\n",
        "    'lasso', lasso_alphas, X_train_scaled, y_train, X_test_scaled, y_test\n",
        ")\n",
        "\n",
        "# --- 7. Comparación y Resultados Finales ---\n",
        "\n",
        "print(\"\\n--- Resumen de los Mejores Modelos Regularizados ---\")\n",
        "print(f\"Mejor Ridge: Alpha={best_alpha_ridge}, R²={best_r2_ridge:.4f}, MSE={best_mse_ridge:.4f}\")\n",
        "print(f\"Mejor Lasso: Alpha={best_alpha_lasso}, R²={best_r2_lasso:.4f}, MSE={best_mse_lasso:.4f}\")\n",
        "\n",
        "# (Opcional) Entrenar un modelo de Regresión Lineal simple como baseline\n",
        "print(\"\\n--- Evaluando Regresión Lineal Simple (Baseline) ---\")\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "print(f\"Regresión Lineal Simple: R²={r2_lr:.4f}, MSE={mse_lr:.4f}\")\n",
        "\n",
        "# (Opcional) Ver los coeficientes del mejor modelo Lasso\n",
        "if best_lasso_model:\n",
        "    print(f\"\\n--- Coeficientes del Mejor Modelo Lasso (Alpha={best_alpha_lasso}) ---\")\n",
        "    lasso_coeffs = pd.DataFrame({\n",
        "        'Característica': X.columns,\n",
        "        'Coeficiente': best_lasso_model.coef_\n",
        "    })\n",
        "    # Mostrar coeficientes no nulos (Lasso realiza selección de características)\n",
        "    print(lasso_coeffs[lasso_coeffs['Coeficiente'] != 0].sort_values(by='Coeficiente', ascending=False))\n",
        "\n",
        "# (Opcional) Ver los coeficientes del mejor modelo Ridge\n",
        "if best_ridge_model:\n",
        "     print(f\"\\n--- Coeficientes del Mejor Modelo Ridge (Alpha={best_alpha_ridge}) ---\")\n",
        "     ridge_coeffs = pd.DataFrame({\n",
        "        'Característica': X.columns,\n",
        "        'Coeficiente': best_ridge_model.coef_\n",
        "     })\n",
        "     print(ridge_coeffs.sort_values(by='Coeficiente', ascending=False))\n",
        "\n",
        "print(\"\\n¡Análisis completado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnDQe02XGVik",
        "outputId": "878aadba-650b-4b30-8fde-70899ceea187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset cargado. Primeras filas:\n",
            "                                        playlist_url  year  \\\n",
            "0  https://open.spotify.com/playlist/37i9dQZF1DWU...  2000   \n",
            "1  https://open.spotify.com/playlist/37i9dQZF1DWU...  2000   \n",
            "2  https://open.spotify.com/playlist/37i9dQZF1DWU...  2000   \n",
            "3  https://open.spotify.com/playlist/37i9dQZF1DWU...  2000   \n",
            "4  https://open.spotify.com/playlist/37i9dQZF1DWU...  2000   \n",
            "\n",
            "                 track_id            track_name  track_popularity  \\\n",
            "0  3AJwUDP919kvQ9QcozQPxg                Yellow                91   \n",
            "1  2m1hi0nfMR9vdGC8UcrnwU  All The Small Things                84   \n",
            "2  3y4LxiYMgDl4RethdzpmNe               Breathe                69   \n",
            "3  60a0Rd6pjrkxjPbaKzXjfq            In the End                88   \n",
            "4  62bOmKYxYg7dhrC6gH9vFn           Bye Bye Bye                74   \n",
            "\n",
            "                           album               artist_id  artist_name  \\\n",
            "0                     Parachutes  4gzpq5DPGxSnKTe4SA8HAU     Coldplay   \n",
            "1             Enema Of The State  6FBDaR13swtiWwGhX1WQsP    blink-182   \n",
            "2                        Breathe  25NQNriVT2YbSW80ILRWJa   Faith Hill   \n",
            "3  Hybrid Theory (Bonus Edition)  6XyY86QOPPrYVGvF9ch6wz  Linkin Park   \n",
            "4            No Strings Attached  6Ff53KvcvAj5U7Z1vojB5o       *NSYNC   \n",
            "\n",
            "                                       artist_genres  artist_popularity  ...  \\\n",
            "0                          ['permanent wave', 'pop']                 86  ...   \n",
            "1  ['alternative metal', 'modern rock', 'pop punk...                 75  ...   \n",
            "2  ['contemporary country', 'country', 'country d...                 61  ...   \n",
            "3  ['alternative metal', 'nu metal', 'post-grunge...                 83  ...   \n",
            "4                   ['boy band', 'dance pop', 'pop']                 65  ...   \n",
            "\n",
            "   loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
            "0    -7.227   1.0       0.0281       0.00239          0.000121    0.2340   \n",
            "1    -4.918   1.0       0.0488       0.01030          0.000000    0.6120   \n",
            "2    -9.007   1.0       0.0290       0.17300          0.000000    0.2510   \n",
            "3    -5.870   0.0       0.0584       0.00958          0.000000    0.2090   \n",
            "4    -4.843   0.0       0.0479       0.03100          0.001200    0.0821   \n",
            "\n",
            "   valence    tempo  duration_ms  time_signature  \n",
            "0    0.285  173.372     266773.0             4.0  \n",
            "1    0.684  148.726     167067.0             4.0  \n",
            "2    0.278  136.859     250547.0             4.0  \n",
            "3    0.400  105.143     216880.0             4.0  \n",
            "4    0.861  172.638     200400.0             4.0  \n",
            "\n",
            "[5 rows x 23 columns]\n",
            "\n",
            "Información del Dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2300 entries, 0 to 2299\n",
            "Data columns (total 23 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   playlist_url       2300 non-null   object \n",
            " 1   year               2300 non-null   int64  \n",
            " 2   track_id           2300 non-null   object \n",
            " 3   track_name         2300 non-null   object \n",
            " 4   track_popularity   2300 non-null   int64  \n",
            " 5   album              2300 non-null   object \n",
            " 6   artist_id          2300 non-null   object \n",
            " 7   artist_name        2300 non-null   object \n",
            " 8   artist_genres      2300 non-null   object \n",
            " 9   artist_popularity  2300 non-null   int64  \n",
            " 10  danceability       2299 non-null   float64\n",
            " 11  energy             2299 non-null   float64\n",
            " 12  key                2299 non-null   float64\n",
            " 13  loudness           2299 non-null   float64\n",
            " 14  mode               2299 non-null   float64\n",
            " 15  speechiness        2299 non-null   float64\n",
            " 16  acousticness       2299 non-null   float64\n",
            " 17  instrumentalness   2299 non-null   float64\n",
            " 18  liveness           2299 non-null   float64\n",
            " 19  valence            2299 non-null   float64\n",
            " 20  tempo              2299 non-null   float64\n",
            " 21  duration_ms        2299 non-null   float64\n",
            " 22  time_signature     2299 non-null   float64\n",
            "dtypes: float64(13), int64(3), object(7)\n",
            "memory usage: 413.4+ KB\n",
            "\n",
            "Eliminando columnas: ['playlist_url', 'track_id', 'track_name', 'album', 'artist_id', 'artist_name', 'artist_genres']\n",
            "\n",
            "Se encontraron valores faltantes en las características. Imputando con la media...\n",
            "Imputación completada.\n",
            "\n",
            "Variable Objetivo (y): track_popularity\n",
            "Características (X) iniciales: ['year', 'artist_popularity', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature']\n",
            "\n",
            "Características Numéricas identificadas: ['year', 'artist_popularity', 'danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms']\n",
            "Características Categóricas identificadas: ['key', 'mode', 'time_signature']\n",
            "\n",
            "División de datos: 1840 entrenamiento, 460 prueba\n",
            "\n",
            "Entrenando LassoCV...\n",
            "Mejor alpha encontrado para Lasso: 0.2420\n",
            "\n",
            "--- Resultados Lasso ---\n",
            "RMSE (Test): 13.8419\n",
            "R² (Test): 0.0531\n",
            "\n",
            "Coeficientes Lasso (los no cero son los más 'importantes' según Lasso):\n",
            "             Feature  Coefficient\n",
            "1  artist_popularity     3.489566\n",
            "0               year     1.035095\n",
            "5        speechiness    -0.221939\n",
            "6       acousticness     0.141243\n",
            "8           liveness    -0.073746\n",
            "3             energy    -0.021837\n",
            "\n",
            "Entrenando RidgeCV...\n",
            "Mejor alpha encontrado para Ridge: 145.0829\n",
            "\n",
            "--- Resultados Ridge ---\n",
            "RMSE (Test): 13.8803\n",
            "R² (Test): 0.0478\n",
            "\n",
            "--- Comparación Final (Test Set) ---\n",
            "Lasso: RMSE=13.8419, R²=0.0531, Alpha=0.2420\n",
            "Ridge: RMSE=13.8803, R²=0.0478, Alpha=145.0829\n",
            "\n",
            "Lasso parece tener un mejor rendimiento (menor RMSE).\n",
            "Lasso explica una mayor proporción de la varianza (mayor R²).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LassoCV, RidgeCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', category=FutureWarning) # Ignorar warnings futuros de scikit-learn\n",
        "\n",
        "# --- 1. Cargar y Preparar los Datos ---\n",
        "\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(\"Dataset cargado. Primeras filas:\")\n",
        "print(df.head())\n",
        "print(\"\\nInformación del Dataset:\")\n",
        "df.info()\n",
        "\n",
        "# --- Selección de Características y Objetivo ---\n",
        "# Columnas potencialmente irrelevantes o identificadores\n",
        "cols_to_drop = ['playlist_url', 'track_id', 'track_name', 'album', 'artist_id', 'artist_name', 'artist_genres']\n",
        "# Nota: 'artist_genres' es complejo. Podría requerir NLP o dividir/contar géneros.\n",
        "#       Lo omitimos por simplicidad inicial. Si es importante, necesitaría un tratamiento especial.\n",
        "\n",
        "# Asegúrate de que las columnas a eliminar existen en el DataFrame\n",
        "existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
        "if len(existing_cols_to_drop) > 0:\n",
        "    print(f\"\\nEliminando columnas: {existing_cols_to_drop}\")\n",
        "    df_processed = df.drop(columns=existing_cols_to_drop)\n",
        "else:\n",
        "    df_processed = df.copy()\n",
        "\n",
        "\n",
        "# Definir variable objetivo (y) y características (X)\n",
        "target = 'track_popularity'\n",
        "\n",
        "# Asegúrate de que la columna objetivo existe\n",
        "if target not in df_processed.columns:\n",
        "    print(f\"Error: La columna objetivo '{target}' no se encuentra en el DataFrame después de eliminar columnas.\")\n",
        "    print(f\"Columnas disponibles: {df_processed.columns.tolist()}\")\n",
        "    exit()\n",
        "\n",
        "y = df_processed[target]\n",
        "X = df_processed.drop(columns=[target])\n",
        "\n",
        "# --- 4. Manejo de Valores Faltantes (Imputar con la media) ---\n",
        "if X.isnull().sum().sum() > 0:\n",
        "    print(\"\\nSe encontraron valores faltantes en las características. Imputando con la media...\")\n",
        "    X = X.apply(lambda col: col.fillna(col.mean()), axis=0)\n",
        "    print(\"Imputación completada.\")\n",
        "else:\n",
        "    print(\"\\nNo se encontraron valores faltantes en las características seleccionadas.\")\n",
        "\n",
        "\n",
        "print(f\"\\nVariable Objetivo (y): {target}\")\n",
        "print(f\"Características (X) iniciales: {X.columns.tolist()}\")\n",
        "\n",
        "# --- Identificar Tipos de Columnas para Preprocesamiento ---\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "# A veces, columnas como 'key', 'mode', 'time_signature' pueden ser numéricas pero deben tratarse como categóricas\n",
        "# Ajusta esto según tu dataset específico\n",
        "potential_cats_numeric = ['key', 'mode', 'time_signature', 'year'] # 'year' puede ser numérico o categórico\n",
        "for col in potential_cats_numeric:\n",
        "    if col in numerical_features:\n",
        "        # Decide si tratarla como categórica\n",
        "        # Por ejemplo, si 'key' tiene pocos valores únicos (0-11), es categórica.\n",
        "        if X[col].nunique() < 15: # Umbral arbitrario\n",
        "             if col not in categorical_features:\n",
        "                 categorical_features.append(col)\n",
        "             numerical_features.remove(col)\n",
        "\n",
        "print(f\"\\nCaracterísticas Numéricas identificadas: {numerical_features}\")\n",
        "print(f\"Características Categóricas identificadas: {categorical_features}\")\n",
        "\n",
        "# --- Crear Pipeline de Preprocesamiento ---\n",
        "# Para características numéricas: escalar\n",
        "# Para características categóricas: One-Hot Encode (ignora errores si encuentra categorías no vistas en test)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough' # Mantener otras columnas si las hubiera (aunque deberíamos haberlas tratado)\n",
        ")\n",
        "\n",
        "\n",
        "# --- Dividir Datos ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nDivisión de datos: {X_train.shape[0]} entrenamiento, {X_test.shape[0]} prueba\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- 2. Regresión Lasso con Búsqueda de Alpha ---\n",
        "\n",
        "# Definir un rango de alphas (logarítmico es común)\n",
        "alphas_lasso = np.logspace(-4, 1, 100) # Rango de 0.0001 a 10\n",
        "\n",
        "# Crear pipeline: Preprocesamiento + Modelo LassoCV\n",
        "# LassoCV usa validación cruzada interna para encontrar el mejor alpha\n",
        "lasso_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regression', LassoCV(alphas=alphas_lasso, cv=5, random_state=42, max_iter=10000, tol=0.001)) # cv=5 -> 5-fold CV\n",
        "])\n",
        "\n",
        "print(\"\\nEntrenando LassoCV...\")\n",
        "lasso_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Mejor alpha encontrado por LassoCV\n",
        "best_alpha_lasso = lasso_pipeline.named_steps['regression'].alpha_\n",
        "print(f\"Mejor alpha encontrado para Lasso: {best_alpha_lasso:.4f}\")\n",
        "\n",
        "# Evaluar en el conjunto de prueba\n",
        "y_pred_lasso = lasso_pipeline.predict(X_test)\n",
        "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
        "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "print(\"\\n--- Resultados Lasso ---\")\n",
        "print(f\"RMSE (Test): {rmse_lasso:.4f}\")\n",
        "print(f\"R² (Test): {r2_lasso:.4f}\")\n",
        "\n",
        "# Opcional: Ver coeficientes (Lasso tiende a poner coeficientes a cero)\n",
        "# Necesitamos obtener los nombres de las características después del OneHotEncoding\n",
        "try:\n",
        "    feature_names = numerical_features + \\\n",
        "                    list(lasso_pipeline.named_steps['preprocess']\n",
        "                         .named_transformers_['cat']\n",
        "                         .get_feature_names_out(categorical_features))\n",
        "    lasso_coefs = lasso_pipeline.named_steps['regression'].coef_\n",
        "    coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': lasso_coefs})\n",
        "    print(\"\\nCoeficientes Lasso (los no cero son los más 'importantes' según Lasso):\")\n",
        "    print(coef_df[coef_df['Coefficient'] != 0].sort_values(by='Coefficient', key=abs, ascending=False).head(10))\n",
        "except Exception as e:\n",
        "    print(f\"\\nNo se pudieron obtener los nombres de las características para los coeficientes: {e}\")\n",
        "\n",
        "\n",
        "# --- 3. Regresión Ridge con Búsqueda de Alpha ---\n",
        "\n",
        "# Definir un rango de alphas\n",
        "alphas_ridge = np.logspace(-3, 4, 100) # Rango de 0.001 a 10000\n",
        "\n",
        "# Crear pipeline: Preprocesamiento + Modelo RidgeCV\n",
        "# RidgeCV también usa CV interna\n",
        "ridge_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    # Usamos neg_mean_squared_error porque RidgeCV maximiza el score, y queremos minimizar MSE\n",
        "    ('regression', RidgeCV(alphas=alphas_ridge, cv=5, scoring='neg_mean_squared_error'))\n",
        "])\n",
        "\n",
        "print(\"\\nEntrenando RidgeCV...\")\n",
        "ridge_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Mejor alpha encontrado por RidgeCV\n",
        "best_alpha_ridge = ridge_pipeline.named_steps['regression'].alpha_\n",
        "print(f\"Mejor alpha encontrado para Ridge: {best_alpha_ridge:.4f}\")\n",
        "\n",
        "# Evaluar en el conjunto de prueba\n",
        "y_pred_ridge = ridge_pipeline.predict(X_test)\n",
        "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "print(\"\\n--- Resultados Ridge ---\")\n",
        "print(f\"RMSE (Test): {rmse_ridge:.4f}\")\n",
        "print(f\"R² (Test): {r2_ridge:.4f}\")\n",
        "\n",
        "# --- 4. Comparar Resultados ---\n",
        "\n",
        "print(\"\\n--- Comparación Final (Test Set) ---\")\n",
        "print(f\"Lasso: RMSE={rmse_lasso:.4f}, R²={r2_lasso:.4f}, Alpha={best_alpha_lasso:.4f}\")\n",
        "print(f\"Ridge: RMSE={rmse_ridge:.4f}, R²={r2_ridge:.4f}, Alpha={best_alpha_ridge:.4f}\")\n",
        "\n",
        "if rmse_lasso < rmse_ridge:\n",
        "    print(\"\\nLasso parece tener un mejor rendimiento (menor RMSE).\")\n",
        "elif rmse_ridge < rmse_lasso:\n",
        "     print(\"\\nRidge parece tener un mejor rendimiento (menor RMSE).\")\n",
        "else:\n",
        "     print(\"\\nLasso y Ridge tienen un rendimiento similar (según RMSE).\")\n",
        "\n",
        "# R²: Más cercano a 1 es mejor.\n",
        "if r2_lasso > r2_ridge:\n",
        "    print(\"Lasso explica una mayor proporción de la varianza (mayor R²).\")\n",
        "elif r2_ridge > r2_lasso:\n",
        "    print(\"Ridge explica una mayor proporción de la varianza (mayor R²).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4cD9a_XHr6z",
        "outputId": "1a2159ba-64c3-4262-b3f2-dd02a017c622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Detectados valores faltantes en las características. Imputando con la media...\n",
            "Imputación completada.\n",
            "\n",
            "Tamaño del conjunto de entrenamiento: (1840, 15)\n",
            "Tamaño del conjunto de prueba: (460, 15)\n",
            "\n",
            "--- Modelo Base: Regresión Lineal ---\n",
            "Mean Squared Error (MSE): 193.0950\n",
            "R-squared (R²): 0.0457\n",
            "\n",
            "--- Buscando Mejor Modelo Regularizado ---\n",
            "Probando Alphas para Lasso: [0.1, 1.0, 10.0]\n",
            "Probando Alphas para Ridge: [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "  Lasso(alpha=0.10): R²=0.0520, MSE=191.8112\n",
            "  Lasso(alpha=1.00): R²=0.0552, MSE=191.1774\n",
            "  Lasso(alpha=10.00): R²=-0.0040, MSE=203.1363\n",
            "  Ridge(alpha=2.00): R²=0.0457, MSE=193.0837\n",
            "  Ridge(alpha=3.00): R²=0.0458, MSE=193.0781\n",
            "  Ridge(alpha=4.00): R²=0.0458, MSE=193.0725\n",
            "  Ridge(alpha=5.00): R²=0.0458, MSE=193.0669\n",
            "  Ridge(alpha=6.00): R²=0.0458, MSE=193.0614\n",
            "  Ridge(alpha=7.00): R²=0.0459, MSE=193.0559\n",
            "  Ridge(alpha=8.00): R²=0.0459, MSE=193.0504\n",
            "  Ridge(alpha=9.00): R²=0.0459, MSE=193.0449\n",
            "  Ridge(alpha=10.00): R²=0.0460, MSE=193.0394\n",
            "\n",
            "--- Mejor Modelo Regularizado Encontrado ---\n",
            "Tipo de Modelo: Lasso\n",
            "Mejor Alpha: 1.0\n",
            "Mejor R² (en test): 0.0552\n",
            "Mejor MSE (en test): 191.1774\n",
            "\n",
            "--- Coeficientes del Mejor Modelo Regularizado ---\n",
            "              Feature  Coefficient\n",
            "1   artist_popularity     2.964994\n",
            "0                year     0.544206\n",
            "2        danceability    -0.000000\n",
            "3              energy    -0.000000\n",
            "4                 key    -0.000000\n",
            "5            loudness    -0.000000\n",
            "6                mode     0.000000\n",
            "7         speechiness    -0.000000\n",
            "8        acousticness     0.000000\n",
            "9    instrumentalness     0.000000\n",
            "10           liveness    -0.000000\n",
            "11            valence    -0.000000\n",
            "12              tempo    -0.000000\n",
            "13        duration_ms    -0.000000\n",
            "14     time_signature    -0.000000\n",
            "\n",
            "Características con coeficiente cero (eliminadas por Lasso): 13\n",
            "\n",
            "--- Resumen Final ---\n",
            "Regresión Lineal Base: R²=0.0457, MSE=193.0950\n",
            "Mejor Modelo Regularizado (Lasso, alpha=1.0): R²=0.0552, MSE=191.1774\n",
            "Mejora en R² respecto a Regresión Lineal: +0.0095\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "\n",
        "# Ignorar advertencias para mantener la salida limpia (opcional)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# --- 2. Preprocesamiento de Datos ---\n",
        "\n",
        "# Seleccionar características (features) y variable objetivo (target)\n",
        "# Excluimos columnas no numéricas o identificadores que no usaremos directamente\n",
        "# 'artist_genres' es complejo (lista/string), lo excluimos por simplicidad inicial.\n",
        "# Podría requerir técnicas más avanzadas (como TF-IDF o embeddings) si se quisiera usar.\n",
        "features = [\n",
        "    'year', 'artist_popularity', 'danceability', 'energy', 'key',\n",
        "    'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
        "    'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature'\n",
        "]\n",
        "target = 'track_popularity'\n",
        "\n",
        "# Verificar si todas las columnas necesarias existen\n",
        "missing_cols = [col for col in features + [target] if col not in df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"\\nError: Faltan las siguientes columnas en el dataset: {missing_cols}\")\n",
        "    exit()\n",
        "\n",
        "# Crear X (features) e y (target)\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Manejar valores faltantes (si los hay) - Estrategia simple: imputar con la media\n",
        "# Podrías elegir otras estrategias (mediana, eliminar filas, etc.)\n",
        "if X.isnull().sum().sum() > 0:\n",
        "    print(\"\\nDetectados valores faltantes en las características. Imputando con la media...\")\n",
        "    for col in X.columns:\n",
        "        if X[col].isnull().any():\n",
        "            mean_val = X[col].mean()\n",
        "            X[col] = X[col].fillna(mean_val)\n",
        "    print(\"Imputación completada.\")\n",
        "\n",
        "if y.isnull().sum() > 0:\n",
        "    print(\"\\nDetectados valores faltantes en la variable objetivo. Eliminando filas...\")\n",
        "    original_len = len(df)\n",
        "    df_cleaned = df.dropna(subset=[target])\n",
        "    X = df_cleaned[features]\n",
        "    y = df_cleaned[target]\n",
        "    # Re-imputar X por si acaso la eliminación de filas en y introdujo NaNs en X de nuevo (poco probable aquí)\n",
        "    if X.isnull().sum().sum() > 0:\n",
        "        for col in X.columns:\n",
        "             if X[col].isnull().any():\n",
        "                mean_val = X[col].mean()\n",
        "                X[col] = X[col].fillna(mean_val)\n",
        "    print(f\"Se eliminaron {original_len - len(df_cleaned)} filas con valor objetivo faltante.\")\n",
        "\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTamaño del conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Tamaño del conjunto de prueba: {X_test.shape}\")\n",
        "\n",
        "# Escalar las características numéricas\n",
        "# Es crucial para modelos regularizados (Lasso, Ridge)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convertir de nuevo a DataFrame para facilitar la visualización (opcional)\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=features)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=features)\n",
        "\n",
        "# --- 3. Modelo Base: Regresión Lineal ---\n",
        "print(\"\\n--- Modelo Base: Regresión Lineal ---\")\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicciones\n",
        "y_pred_lr = lr_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluación\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse_lr:.4f}\")\n",
        "print(f\"R-squared (R²): {r2_lr:.4f}\")\n",
        "\n",
        "# --- 4. Búsqueda del Mejor Modelo Regularizado (Lasso/Ridge) ---\n",
        "\n",
        "def find_best_regularized_model(X_train, y_train, X_test, y_test, lasso_alphas, ridge_alphas):\n",
        "    \"\"\"\n",
        "    Entrena modelos Lasso y Ridge con diferentes alphas y encuentra el mejor.\n",
        "\n",
        "    Args:\n",
        "        X_train: Características de entrenamiento (escaladas).\n",
        "        y_train: Variable objetivo de entrenamiento.\n",
        "        X_test: Características de prueba (escaladas).\n",
        "        y_test: Variable objetivo de prueba.\n",
        "        lasso_alphas: Lista o iterable de valores alpha para probar en Lasso.\n",
        "        ridge_alphas: Lista o iterable de valores alpha para probar en Ridge.\n",
        "\n",
        "    Returns:\n",
        "        Un diccionario con la información del mejor modelo encontrado:\n",
        "        {'model_type': 'Lasso' o 'Ridge', 'best_alpha': mejor valor alpha,\n",
        "         'best_r2_score': R² en el conjunto de prueba, 'best_mse': MSE en el conjunto de prueba,\n",
        "         'best_model': el objeto del modelo sklearn entrenado}\n",
        "    \"\"\"\n",
        "    best_model_info = {\n",
        "        'model_type': None,\n",
        "        'best_alpha': None,\n",
        "        'best_r2_score': -np.inf, # Inicializar con un valor muy bajo para R²\n",
        "        'best_mse': np.inf,      # Inicializar con un valor muy alto para MSE\n",
        "        'best_model': None\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Buscando Mejor Modelo Regularizado ---\")\n",
        "    print(f\"Probando Alphas para Lasso: {list(lasso_alphas)}\")\n",
        "    print(f\"Probando Alphas para Ridge: {list(ridge_alphas)}\")\n",
        "\n",
        "    # Probar modelos Lasso\n",
        "    for alpha in lasso_alphas:\n",
        "        lasso = Lasso(alpha=alpha, random_state=42, max_iter=10000) # Aumentar max_iter si no converge\n",
        "        lasso.fit(X_train, y_train)\n",
        "        y_pred = lasso.predict(X_test)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        print(f\"  Lasso(alpha={alpha:.2f}): R²={r2:.4f}, MSE={mse:.4f}\")\n",
        "\n",
        "        if r2 > best_model_info['best_r2_score']:\n",
        "            best_model_info['model_type'] = 'Lasso'\n",
        "            best_model_info['best_alpha'] = alpha\n",
        "            best_model_info['best_r2_score'] = r2\n",
        "            best_model_info['best_mse'] = mse\n",
        "            best_model_info['best_model'] = lasso\n",
        "\n",
        "    # Probar modelos Ridge\n",
        "    for alpha in ridge_alphas:\n",
        "        ridge = Ridge(alpha=alpha, random_state=42)\n",
        "        ridge.fit(X_train, y_train)\n",
        "        y_pred = ridge.predict(X_test)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        print(f\"  Ridge(alpha={alpha:.2f}): R²={r2:.4f}, MSE={mse:.4f}\")\n",
        "\n",
        "        if r2 > best_model_info['best_r2_score']:\n",
        "            best_model_info['model_type'] = 'Ridge'\n",
        "            best_model_info['best_alpha'] = alpha\n",
        "            best_model_info['best_r2_score'] = r2\n",
        "            best_model_info['best_mse'] = mse\n",
        "            best_model_info['best_model'] = ridge\n",
        "\n",
        "    print(\"\\n--- Mejor Modelo Regularizado Encontrado ---\")\n",
        "    if best_model_info['best_model']:\n",
        "        print(f\"Tipo de Modelo: {best_model_info['model_type']}\")\n",
        "        print(f\"Mejor Alpha: {best_model_info['best_alpha']}\")\n",
        "        print(f\"Mejor R² (en test): {best_model_info['best_r2_score']:.4f}\")\n",
        "        print(f\"Mejor MSE (en test): {best_model_info['best_mse']:.4f}\")\n",
        "    else:\n",
        "        print(\"No se encontró un modelo mejor que la inicialización.\")\n",
        "\n",
        "    return best_model_info\n",
        "\n",
        "# Definir los rangos de alpha a probar según tu solicitud\n",
        "# Ridge: 2 a 10 (incluidos)\n",
        "ridge_alpha_range = range(2, 11) # range(start, stop) -> stop is exclusive, so use 11\n",
        "# Lasso: 0.1, 1, 10\n",
        "lasso_alpha_range = [0.1, 1.0, 10.0]\n",
        "\n",
        "# Ejecutar la función de búsqueda\n",
        "best_regularized_model_info = find_best_regularized_model(\n",
        "    X_train_scaled, y_train, X_test_scaled, y_test,\n",
        "    lasso_alpha_range, ridge_alpha_range\n",
        ")\n",
        "\n",
        "# --- 5. Análisis de Coeficientes (Opcional) ---\n",
        "if best_regularized_model_info['best_model']:\n",
        "    print(\"\\n--- Coeficientes del Mejor Modelo Regularizado ---\")\n",
        "    best_model = best_regularized_model_info['best_model']\n",
        "    coefficients = pd.DataFrame({\n",
        "        'Feature': features,\n",
        "        'Coefficient': best_model.coef_\n",
        "    })\n",
        "    # Ordenar por valor absoluto del coeficiente para ver las características más influyentes\n",
        "    coefficients['Abs_Coefficient'] = coefficients['Coefficient'].abs()\n",
        "    coefficients = coefficients.sort_values(by='Abs_Coefficient', ascending=False).drop('Abs_Coefficient', axis=1)\n",
        "    print(coefficients)\n",
        "\n",
        "    # En Lasso, algunos coeficientes pueden ser exactamente cero\n",
        "    if best_regularized_model_info['model_type'] == 'Lasso':\n",
        "        zero_coeffs = coefficients[coefficients['Coefficient'] == 0]\n",
        "        print(f\"\\nCaracterísticas con coeficiente cero (eliminadas por Lasso): {len(zero_coeffs)}\")\n",
        "        # print(zero_coeffs['Feature'].tolist()) # Descomentar si quieres ver la lista\n",
        "else:\n",
        "    print(\"\\nNo se puede mostrar coeficientes ya que no se seleccionó un mejor modelo regularizado.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Resumen Final ---\")\n",
        "print(f\"Regresión Lineal Base: R²={r2_lr:.4f}, MSE={mse_lr:.4f}\")\n",
        "if best_regularized_model_info['best_model']:\n",
        "    print(f\"Mejor Modelo Regularizado ({best_regularized_model_info['model_type']}, alpha={best_regularized_model_info['best_alpha']}): R²={best_regularized_model_info['best_r2_score']:.4f}, MSE={best_regularized_model_info['best_mse']:.4f}\")\n",
        "    improvement = best_regularized_model_info['best_r2_score'] - r2_lr\n",
        "    print(f\"Mejora en R² respecto a Regresión Lineal: {improvement:+.4f}\")\n",
        "else:\n",
        "    print(\"No se encontró un modelo regularizado que mejorara el R² inicial.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbEyCEJSg5pd"
      },
      "source": [
        "### **Regresion poli**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-K6dxDDg5OO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHRPq1m1WNP1",
        "outputId": "3132e330-43dc-43d7-ea12-304e94a4c412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: No se encontró el archivo en la ruta '/content/your_dataset.csv'.\n",
            "Por favor, sube tu archivo a Google Colab y verifica la ruta.\n",
            "\n",
            "Forma original: (2300, 23)\n",
            "Forma después de eliminar NaNs: (2299, 23)\n",
            "Se eliminaron 1 filas con valores faltantes.\n",
            "\n",
            "Tamaño del conjunto de entrenamiento: (1839, 15)\n",
            "Tamaño del conjunto de prueba: (460, 15)\n",
            "\n",
            "--- Modelo Base: Regresión Lineal Simple ---\n",
            "Mean Squared Error (MSE): 148.2121\n",
            "R-squared (R²): 0.0832\n",
            "\n",
            "--- Buscando Mejor Modelo Polinómico Regularizado ---\n",
            "Probando Grados Polinómicos: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Probando Alphas: [0.1, 1.0, 10.0]\n",
            "\n",
            "--- Probando Grado Polinómico: 1 ---\n",
            "  Grado=1, Ridge(alpha=0.10): R²=0.0832, MSE=148.2118\n",
            "  Grado=1, Ridge(alpha=1.00): R²=0.0833, MSE=148.2092\n",
            "  Grado=1, Ridge(alpha=10.00): R²=0.0834, MSE=148.1830\n",
            "  Grado=1, Lasso(alpha=0.10): R²=0.0860, MSE=147.7604\n",
            "  Grado=1, Lasso(alpha=1.00): R²=0.0808, MSE=148.6134\n",
            "  Grado=1, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 2 ---\n",
            "  Grado=2, Ridge(alpha=0.10): R²=0.0738, MSE=149.7313\n",
            "  Grado=2, Ridge(alpha=1.00): R²=0.0763, MSE=149.3426\n",
            "  Grado=2, Ridge(alpha=10.00): R²=0.0805, MSE=148.6545\n",
            "  Grado=2, Lasso(alpha=0.10): R²=0.0823, MSE=148.3710\n",
            "  Grado=2, Lasso(alpha=1.00): R²=0.0808, MSE=148.6123\n",
            "  Grado=2, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 3 ---\n",
            "  Grado=3, Ridge(alpha=0.10): R²=-0.6424, MSE=265.5230\n",
            "  Grado=3, Ridge(alpha=1.00): R²=-0.0808, MSE=174.7397\n",
            "  Grado=3, Ridge(alpha=10.00): R²=0.0663, MSE=150.9533\n",
            "  Grado=3, Lasso(alpha=0.10): R²=0.0864, MSE=147.7088\n",
            "  Grado=3, Lasso(alpha=1.00): R²=0.0808, MSE=148.6091\n",
            "  Grado=3, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 4 ---\n",
            "  Grado=4, Ridge(alpha=0.10): R²=-8.1049, MSE=1471.9951\n",
            "  Grado=4, Ridge(alpha=1.00): R²=-1.3514, MSE=380.1538\n",
            "  Grado=4, Ridge(alpha=10.00): R²=-0.1274, MSE=182.2656\n",
            "  Grado=4, Lasso(alpha=0.10): R²=0.1131, MSE=143.3844\n",
            "  Grado=4, Lasso(alpha=1.00): R²=0.0808, MSE=148.6046\n",
            "  Grado=4, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 5 ---\n",
            "  Grado=5, Ridge(alpha=0.10): R²=-96.9340, MSE=15832.9867\n",
            "  Grado=5, Ridge(alpha=1.00): R²=-9.3498, MSE=1673.2550\n",
            "  Grado=5, Ridge(alpha=10.00): R²=-1.4519, MSE=396.3973\n",
            "  Grado=5, Lasso(alpha=0.10): R²=0.1136, MSE=143.2999\n",
            "  Grado=5, Lasso(alpha=1.00): R²=0.0808, MSE=148.6006\n",
            "  Grado=5, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 6 ---\n",
            "  Grado=6, Ridge(alpha=0.10): R²=-349.7929, MSE=56712.7050\n",
            "  Grado=6, Ridge(alpha=1.00): R²=-64.0765, MSE=10520.9186\n",
            "  Grado=6, Ridge(alpha=10.00): R²=-7.2216, MSE=1329.1920\n",
            "  Grado=6, Lasso(alpha=0.10): R²=0.1080, MSE=144.2102\n",
            "  Grado=6, Lasso(alpha=1.00): R²=0.0809, MSE=148.5969\n",
            "  Grado=6, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 7 ---\n",
            "  Grado=7, Ridge(alpha=0.10): R²=-630.6535, MSE=102119.4441\n",
            "  Grado=7, Ridge(alpha=1.00): R²=-249.9903, MSE=40577.6154\n",
            "  Grado=7, Ridge(alpha=10.00): R²=-34.1719, MSE=5686.2412\n",
            "  Grado=7, Lasso(alpha=0.10): R²=0.0969, MSE=146.0044\n",
            "  Grado=7, Lasso(alpha=1.00): R²=0.0810, MSE=148.5808\n",
            "  Grado=7, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 8 ---\n",
            "  Grado=8, Ridge(alpha=0.10): R²=-1102.7254, MSE=178439.3145\n",
            "  Grado=8, Ridge(alpha=1.00): R²=-561.6736, MSE=90967.4537\n",
            "  Grado=8, Ridge(alpha=10.00): R²=-146.9113, MSE=23912.8255\n",
            "  Grado=8, Lasso(alpha=0.10): R²=0.0031, MSE=161.1695\n",
            "  Grado=8, Lasso(alpha=1.00): R²=0.0800, MSE=148.7328\n",
            "  Grado=8, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 9 ---\n",
            "  Grado=9, Ridge(alpha=0.10): R²=-1674.9596, MSE=270952.4519\n",
            "  Grado=9, Ridge(alpha=1.00): R²=-971.3813, MSE=157204.9214\n",
            "  Grado=9, Ridge(alpha=10.00): R²=-428.2353, MSE=69394.4830\n",
            "  Grado=9, Lasso(alpha=0.10): R²=-0.0231, MSE=165.4073\n",
            "  Grado=9, Lasso(alpha=1.00): R²=0.0760, MSE=149.3783\n",
            "  Grado=9, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 10 ---\n",
            "  Grado=10, Ridge(alpha=0.10): R²=-2086.7936, MSE=337533.6547\n",
            "  Grado=10, Ridge(alpha=1.00): R²=-1570.2757, MSE=254028.1876\n",
            "  Grado=10, Ridge(alpha=10.00): R²=-910.2832, MSE=147327.1858\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3a48d9018561>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;31m# Ejecutar la función de búsqueda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;31m# Pasamos los datos SIN escalar, ya que el escalado está dentro del pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m best_poly_reg_model_info = find_best_polynomial_regularized_model(\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mdegrees_to_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas_to_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-3a48d9018561>\u001b[0m in \u001b[0;36mfind_best_polynomial_regularized_model\u001b[0;34m(X_train, y_train, X_test, y_test, degrees, alphas)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;31m# Entrenar el pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0mlasso_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# Predecir en el conjunto de prueba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasso_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mall_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 )\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlast_step_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 \u001b[0mthis_Xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m             _, this_coef, this_dual_gap, this_iter = self.path(\n\u001b[0m\u001b[1;32m   1081\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_coordinate_descent.py\u001b[0m in \u001b[0;36menet_path\u001b[0;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[1;32m    693\u001b[0m             )\n\u001b[1;32m    694\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mprecompute\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             model = cd_fast.enet_coordinate_descent(\n\u001b[0m\u001b[1;32m    696\u001b[0m                 \u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "\n",
        "# Ignorar advertencias para mantener la salida limpia (opcional)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 1. Carga de Datos ---\n",
        "# Asegúrate de subir tu archivo CSV a Google Colab y actualizar la ruta\n",
        "# Ejemplo: '/content/nombre_de_tu_archivo.csv'\n",
        "file_path = '/content/your_dataset.csv' # <-- ¡CAMBIA ESTA RUTA!\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset cargado exitosamente.\")\n",
        "    print(f\"Forma del dataset: {df.shape}\")\n",
        "    # print(\"\\nPrimeras 5 filas:\")\n",
        "    # print(df.head())\n",
        "    # print(\"\\nInformación del dataset:\")\n",
        "    # df.info() # Descomentar si necesitas ver la info detallada\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: No se encontró el archivo en la ruta '{file_path}'.\")\n",
        "    print(\"Por favor, sube tu archivo a Google Colab y verifica la ruta.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Ocurrió un error al cargar el dataset: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Preprocesamiento de Datos ---\n",
        "\n",
        "# Seleccionar características (features) y variable objetivo (target)\n",
        "features = [\n",
        "    'year', 'artist_popularity', 'danceability', 'energy', 'key',\n",
        "    'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
        "    'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature'\n",
        "]\n",
        "target = 'track_popularity'\n",
        "\n",
        "# Verificar si todas las columnas necesarias existen\n",
        "missing_cols = [col for col in features + [target] if col not in df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"\\nError: Faltan las siguientes columnas en el dataset: {missing_cols}\")\n",
        "    exit()\n",
        "\n",
        "# Eliminar filas con valores faltantes en características o target (estrategia simple)\n",
        "# Puedes cambiar esto por imputación si lo prefieres\n",
        "print(f\"\\nForma original: {df.shape}\")\n",
        "df_cleaned = df.dropna(subset=features + [target])\n",
        "print(f\"Forma después de eliminar NaNs: {df_cleaned.shape}\")\n",
        "print(f\"Se eliminaron {df.shape[0] - df_cleaned.shape[0]} filas con valores faltantes.\")\n",
        "\n",
        "if df_cleaned.empty:\n",
        "    print(\"\\nError: El dataset quedó vacío después de eliminar filas con NaNs.\")\n",
        "    exit()\n",
        "\n",
        "X = df_cleaned[features]\n",
        "y = df_cleaned[target]\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTamaño del conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Tamaño del conjunto de prueba: {X_test.shape}\")\n",
        "\n",
        "# --- 3. Modelo Base: Regresión Lineal (Grado Polinómico 1) ---\n",
        "print(\"\\n--- Modelo Base: Regresión Lineal Simple ---\")\n",
        "# Usamos un pipeline para escalar y luego aplicar regresión lineal\n",
        "# Aunque no es estrictamente necesario para LR simple, mantiene la consistencia\n",
        "base_lr_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('linear_regression', LinearRegression())\n",
        "])\n",
        "\n",
        "base_lr_pipe.fit(X_train, y_train)\n",
        "y_pred_lr_base = base_lr_pipe.predict(X_test)\n",
        "mse_lr_base = mean_squared_error(y_test, y_pred_lr_base)\n",
        "r2_lr_base = r2_score(y_test, y_pred_lr_base)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse_lr_base:.4f}\")\n",
        "print(f\"R-squared (R²): {r2_lr_base:.4f}\")\n",
        "\n",
        "# --- 4. Función para Buscar el Mejor Modelo Polinómico Regularizado ---\n",
        "\n",
        "def find_best_polynomial_regularized_model(X_train, y_train, X_test, y_test, degrees, alphas):\n",
        "    \"\"\"\n",
        "    Prueba modelos Lasso y Ridge con diferentes grados polinómicos y alphas.\n",
        "\n",
        "    Args:\n",
        "        X_train: Características de entrenamiento (sin escalar).\n",
        "        y_train: Variable objetivo de entrenamiento.\n",
        "        X_test: Características de prueba (sin escalar).\n",
        "        y_test: Variable objetivo de prueba.\n",
        "        degrees: Lista o iterable de grados polinómicos a probar (ej: range(1, 11)).\n",
        "        alphas: Lista o iterable de valores alpha a probar (ej: [0.1, 1, 10]).\n",
        "\n",
        "    Returns:\n",
        "        Un diccionario con la información del mejor modelo encontrado.\n",
        "    \"\"\"\n",
        "    best_model_info = {\n",
        "        'model_type': None,\n",
        "        'degree': None,\n",
        "        'alpha': None,\n",
        "        'best_r2_score': -np.inf, # Inicializar R² con valor muy bajo\n",
        "        'best_mse': np.inf,      # Inicializar MSE con valor muy alto\n",
        "        'best_pipeline': None    # Guardaremos el pipeline completo\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Buscando Mejor Modelo Polinómico Regularizado ---\")\n",
        "    print(f\"Probando Grados Polinómicos: {list(degrees)}\")\n",
        "    print(f\"Probando Alphas: {list(alphas)}\")\n",
        "\n",
        "    for degree in degrees:\n",
        "        print(f\"\\n--- Probando Grado Polinómico: {degree} ---\")\n",
        "\n",
        "        # Crear el transformador de características polinómicas\n",
        "        # include_bias=False porque el modelo lineal (Lasso/Ridge) ya maneja la intercepción\n",
        "        poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "\n",
        "        # Probar Ridge con este grado\n",
        "        for alpha in alphas:\n",
        "            # Crear el pipeline completo: Poly -> Scaler -> Ridge\n",
        "            ridge_pipe = Pipeline([\n",
        "                ('poly', poly_features),\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('ridge', Ridge(alpha=alpha, random_state=42))\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                # Entrenar el pipeline\n",
        "                ridge_pipe.fit(X_train, y_train)\n",
        "                # Predecir en el conjunto de prueba\n",
        "                y_pred = ridge_pipe.predict(X_test)\n",
        "                # Evaluar\n",
        "                r2 = r2_score(y_test, y_pred)\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                print(f\"  Grado={degree}, Ridge(alpha={alpha:.2f}): R²={r2:.4f}, MSE={mse:.4f}\")\n",
        "\n",
        "                # Actualizar si es el mejor modelo hasta ahora\n",
        "                if r2 > best_model_info['best_r2_score']:\n",
        "                    best_model_info['model_type'] = 'Ridge'\n",
        "                    best_model_info['degree'] = degree\n",
        "                    best_model_info['alpha'] = alpha\n",
        "                    best_model_info['best_r2_score'] = r2\n",
        "                    best_model_info['best_mse'] = mse\n",
        "                    best_model_info['best_pipeline'] = ridge_pipe\n",
        "\n",
        "            except MemoryError:\n",
        "                 print(f\"  Grado={degree}, Ridge(alpha={alpha:.2f}): Error de Memoria - Demasiadas características polinómicas. Saltando.\")\n",
        "                 # Si un grado causa error de memoria, probablemente los siguientes también lo harán para este alpha\n",
        "                 # Podríamos romper el bucle interno de alpha aquí si es necesario\n",
        "                 continue # Pasar al siguiente alpha o grado\n",
        "            except Exception as e:\n",
        "                 print(f\"  Grado={degree}, Ridge(alpha={alpha:.2f}): Error inesperado - {e}. Saltando.\")\n",
        "                 continue\n",
        "\n",
        "        # Probar Lasso con este grado\n",
        "        for alpha in alphas:\n",
        "             # Crear el pipeline completo: Poly -> Scaler -> Lasso\n",
        "            lasso_pipe = Pipeline([\n",
        "                ('poly', poly_features),\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('lasso', Lasso(alpha=alpha, random_state=42, max_iter=10000, tol=0.01)) # Aumentar max_iter y tol puede ayudar a la convergencia\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                # Entrenar el pipeline\n",
        "                lasso_pipe.fit(X_train, y_train)\n",
        "                # Predecir en el conjunto de prueba\n",
        "                y_pred = lasso_pipe.predict(X_test)\n",
        "                # Evaluar\n",
        "                r2 = r2_score(y_test, y_pred)\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                print(f\"  Grado={degree}, Lasso(alpha={alpha:.2f}): R²={r2:.4f}, MSE={mse:.4f}\")\n",
        "\n",
        "                # Actualizar si es el mejor modelo hasta ahora\n",
        "                if r2 > best_model_info['best_r2_score']:\n",
        "                    best_model_info['model_type'] = 'Lasso'\n",
        "                    best_model_info['degree'] = degree\n",
        "                    best_model_info['alpha'] = alpha\n",
        "                    best_model_info['best_r2_score'] = r2\n",
        "                    best_model_info['best_mse'] = mse\n",
        "                    best_model_info['best_pipeline'] = lasso_pipe\n",
        "\n",
        "            except MemoryError:\n",
        "                 print(f\"  Grado={degree}, Lasso(alpha={alpha:.2f}): Error de Memoria - Demasiadas características polinómicas. Saltando.\")\n",
        "                 continue\n",
        "            except Exception as e:\n",
        "                 print(f\"  Grado={degree}, Lasso(alpha={alpha:.2f}): Error inesperado - {e}. Saltando.\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "    print(\"\\n--- Mejor Modelo Polinómico Regularizado Encontrado ---\")\n",
        "    if best_model_info['best_pipeline']:\n",
        "        print(f\"Tipo de Modelo: {best_model_info['model_type']}\")\n",
        "        print(f\"Grado Polinómico: {best_model_info['degree']}\")\n",
        "        print(f\"Mejor Alpha: {best_model_info['alpha']}\")\n",
        "        print(f\"Mejor R² (en test): {best_model_info['best_r2_score']:.4f}\")\n",
        "        print(f\"Mejor MSE (en test): {best_model_info['best_mse']:.4f}\")\n",
        "    else:\n",
        "        print(\"No se encontró un modelo que mejorara la inicialización (posiblemente debido a errores).\")\n",
        "\n",
        "    return best_model_info\n",
        "\n",
        "# --- 5. Ejecutar la Búsqueda ---\n",
        "\n",
        "# Definir los rangos a probar según tu solicitud\n",
        "degrees_to_test = range(1, 11) # Grados del 1 al 10\n",
        "alphas_to_test = [0.1, 1.0, 10.0] # Alphas 0.1, 1, 10\n",
        "\n",
        "# Ejecutar la función de búsqueda\n",
        "# Pasamos los datos SIN escalar, ya que el escalado está dentro del pipeline\n",
        "best_poly_reg_model_info = find_best_polynomial_regularized_model(\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    degrees_to_test, alphas_to_test\n",
        ")\n",
        "\n",
        "# --- 6. Resumen Final ---\n",
        "print(\"\\n--- Resumen Final de Rendimiento (en conjunto de prueba) ---\")\n",
        "print(f\"Regresión Lineal Simple (Grado 1): R²={r2_lr_base:.4f}, MSE={mse_lr_base:.4f}\")\n",
        "\n",
        "if best_poly_reg_model_info['best_pipeline']:\n",
        "    print(f\"Mejor Modelo Encontrado ({best_poly_reg_model_info['model_type']}, Grado={best_poly_reg_model_info['degree']}, Alpha={best_poly_reg_model_info['alpha']}): R²={best_poly_reg_model_info['best_r2_score']:.4f}, MSE={best_poly_reg_model_info['best_mse']:.4f}\")\n",
        "    improvement_r2 = best_poly_reg_model_info['best_r2_score'] - r2_lr_base\n",
        "    print(f\"Mejora en R² respecto a Regresión Lineal Simple: {improvement_r2:+.4f}\")\n",
        "else:\n",
        "    print(\"No se encontró un modelo polinómico regularizado óptimo en la búsqueda (comparado con R²=-inf).\")\n",
        "\n",
        "# --- 7. (Opcional) Análisis de Coeficientes del Mejor Modelo ---\n",
        "# La interpretación es más compleja con características polinómicas\n",
        "if best_poly_reg_model_info['best_pipeline']:\n",
        "    best_pipeline = best_poly_reg_model_info['best_pipeline']\n",
        "    try:\n",
        "        # Obtener los nombres de las características polinómicas generadas\n",
        "        poly_step = best_pipeline.named_steps['poly']\n",
        "        model_step = best_pipeline.steps[-1][1] # El último paso es el modelo (Lasso o Ridge)\n",
        "\n",
        "        feature_names_poly = poly_step.get_feature_names_out(input_features=features)\n",
        "\n",
        "        coefficients = pd.DataFrame({\n",
        "            'Feature': feature_names_poly,\n",
        "            'Coefficient': model_step.coef_\n",
        "        })\n",
        "        coefficients['Abs_Coefficient'] = coefficients['Coefficient'].abs()\n",
        "        coefficients = coefficients.sort_values(by='Abs_Coefficient', ascending=False).drop('Abs_Coefficient', axis=1)\n",
        "\n",
        "        print(f\"\\n--- Coeficientes del Mejor Modelo ({best_poly_reg_model_info['model_type']}, Grado={best_poly_reg_model_info['degree']}, Alpha={best_poly_reg_model_info['alpha']}) ---\")\n",
        "        print(coefficients.head(15)) # Mostrar solo los N más importantes por claridad\n",
        "\n",
        "        if best_poly_reg_model_info['model_type'] == 'Lasso':\n",
        "            zero_coeffs = coefficients[np.isclose(coefficients['Coefficient'], 0)] # Usar isclose por precisión flotante\n",
        "            print(f\"\\nCaracterísticas con coeficiente cercano a cero (potencialmente eliminadas por Lasso): {len(zero_coeffs)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nNo se pudieron extraer los coeficientes del mejor modelo: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ywl1oGeswT7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYIOLoOch9JK",
        "outputId": "3f067a63-916c-4fd0-e6f2-9861dab70fec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Forma original: (2300, 23)\n",
            "Forma después de eliminar NaNs: (2299, 23)\n",
            "Se eliminaron 1 filas con valores faltantes.\n",
            "\n",
            "Tamaño del conjunto de entrenamiento: (1839, 15)\n",
            "Tamaño del conjunto de prueba: (460, 15)\n",
            "\n",
            "--- Modelo Base: Regresión Lineal Simple ---\n",
            "Mean Squared Error (MSE): 148.2121\n",
            "R-squared (R²): 0.0832\n",
            "\n",
            "--- Buscando Mejor Modelo Polinómico Regularizado ---\n",
            "Probando Grados Polinómicos: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Probando Alphas: [0.1, 1.0, 10.0]\n",
            "\n",
            "--- Probando Grado Polinómico: 1 ---\n",
            "  Grado=1, Ridge(alpha=0.10): R²=0.0832, MSE=148.2118\n",
            "  Grado=1, Ridge(alpha=1.00): R²=0.0833, MSE=148.2092\n",
            "  Grado=1, Ridge(alpha=10.00): R²=0.0834, MSE=148.1830\n",
            "  Grado=1, Lasso(alpha=0.10): R²=0.0860, MSE=147.7604\n",
            "  Grado=1, Lasso(alpha=1.00): R²=0.0808, MSE=148.6134\n",
            "  Grado=1, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 2 ---\n",
            "  Grado=2, Ridge(alpha=0.10): R²=0.0738, MSE=149.7313\n",
            "  Grado=2, Ridge(alpha=1.00): R²=0.0763, MSE=149.3426\n",
            "  Grado=2, Ridge(alpha=10.00): R²=0.0805, MSE=148.6545\n",
            "  Grado=2, Lasso(alpha=0.10): R²=0.0823, MSE=148.3710\n",
            "  Grado=2, Lasso(alpha=1.00): R²=0.0808, MSE=148.6123\n",
            "  Grado=2, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 3 ---\n",
            "  Grado=3, Ridge(alpha=0.10): R²=-0.6424, MSE=265.5230\n",
            "  Grado=3, Ridge(alpha=1.00): R²=-0.0808, MSE=174.7397\n",
            "  Grado=3, Ridge(alpha=10.00): R²=0.0663, MSE=150.9533\n",
            "  Grado=3, Lasso(alpha=0.10): R²=0.0864, MSE=147.7088\n",
            "  Grado=3, Lasso(alpha=1.00): R²=0.0808, MSE=148.6091\n",
            "  Grado=3, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 4 ---\n",
            "  Grado=4, Ridge(alpha=0.10): R²=-8.1049, MSE=1471.9951\n",
            "  Grado=4, Ridge(alpha=1.00): R²=-1.3514, MSE=380.1538\n",
            "  Grado=4, Ridge(alpha=10.00): R²=-0.1274, MSE=182.2656\n",
            "  Grado=4, Lasso(alpha=0.10): R²=0.1131, MSE=143.3844\n",
            "  Grado=4, Lasso(alpha=1.00): R²=0.0808, MSE=148.6046\n",
            "  Grado=4, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 5 ---\n",
            "  Grado=5, Ridge(alpha=0.10): R²=-96.9340, MSE=15832.9867\n",
            "  Grado=5, Ridge(alpha=1.00): R²=-9.3498, MSE=1673.2550\n",
            "  Grado=5, Ridge(alpha=10.00): R²=-1.4519, MSE=396.3973\n",
            "  Grado=5, Lasso(alpha=0.10): R²=0.1136, MSE=143.2999\n",
            "  Grado=5, Lasso(alpha=1.00): R²=0.0808, MSE=148.6006\n",
            "  Grado=5, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 6 ---\n",
            "  Grado=6, Ridge(alpha=0.10): R²=-349.7929, MSE=56712.7050\n",
            "  Grado=6, Ridge(alpha=1.00): R²=-64.0765, MSE=10520.9186\n",
            "  Grado=6, Ridge(alpha=10.00): R²=-7.2216, MSE=1329.1920\n",
            "  Grado=6, Lasso(alpha=0.10): R²=0.1080, MSE=144.2102\n",
            "  Grado=6, Lasso(alpha=1.00): R²=0.0809, MSE=148.5969\n",
            "  Grado=6, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 7 ---\n",
            "  Grado=7, Ridge(alpha=0.10): R²=-630.6535, MSE=102119.4441\n",
            "  Grado=7, Ridge(alpha=1.00): R²=-249.9903, MSE=40577.6154\n",
            "  Grado=7, Ridge(alpha=10.00): R²=-34.1719, MSE=5686.2412\n",
            "  Grado=7, Lasso(alpha=0.10): R²=0.0969, MSE=146.0044\n",
            "  Grado=7, Lasso(alpha=1.00): R²=0.0810, MSE=148.5808\n",
            "  Grado=7, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 8 ---\n",
            "  Grado=8, Ridge(alpha=0.10): R²=-1102.7254, MSE=178439.3145\n",
            "  Grado=8, Ridge(alpha=1.00): R²=-561.6736, MSE=90967.4537\n",
            "  Grado=8, Ridge(alpha=10.00): R²=-146.9113, MSE=23912.8255\n",
            "  Grado=8, Lasso(alpha=0.10): R²=0.0031, MSE=161.1695\n",
            "  Grado=8, Lasso(alpha=1.00): R²=0.0800, MSE=148.7328\n",
            "  Grado=8, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 9 ---\n",
            "  Grado=9, Ridge(alpha=0.10): R²=-1674.9596, MSE=270952.4519\n",
            "  Grado=9, Ridge(alpha=1.00): R²=-971.3813, MSE=157204.9214\n",
            "  Grado=9, Ridge(alpha=10.00): R²=-428.2353, MSE=69394.4830\n",
            "  Grado=9, Lasso(alpha=0.10): R²=-0.0231, MSE=165.4073\n",
            "  Grado=9, Lasso(alpha=1.00): R²=0.0760, MSE=149.3783\n",
            "  Grado=9, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Probando Grado Polinómico: 10 ---\n",
            "  Grado=10, Ridge(alpha=0.10): R²=-2086.7936, MSE=337533.6547\n",
            "  Grado=10, Ridge(alpha=1.00): R²=-1570.2757, MSE=254028.1876\n",
            "  Grado=10, Ridge(alpha=10.00): R²=-910.2832, MSE=147327.1858\n",
            "  Grado=10, Lasso(alpha=0.10): R²=-0.1203, MSE=181.1215\n",
            "  Grado=10, Lasso(alpha=1.00): R²=0.0728, MSE=149.9062\n",
            "  Grado=10, Lasso(alpha=10.00): R²=-0.0000, MSE=161.6703\n",
            "\n",
            "--- Mejor Modelo Polinómico Regularizado Encontrado ---\n",
            "Tipo de Modelo: Lasso\n",
            "Grado Polinómico: 5\n",
            "Mejor Alpha: 0.1\n",
            "Mejor R² (en test): 0.1136\n",
            "Mejor MSE (en test): 143.2999\n",
            "\n",
            "--- Resumen Final de Rendimiento (en conjunto de prueba) ---\n",
            "Regresión Lineal Simple (Grado 1): R²=0.0832, MSE=148.2121\n",
            "Mejor Modelo Encontrado (Lasso, Grado=5, Alpha=0.1): R²=0.1136, MSE=143.2999\n",
            "Mejora en R² respecto a Regresión Lineal Simple: +0.0304\n",
            "\n",
            "--- Coeficientes del Mejor Modelo (Lasso, Grado=5, Alpha=0.1) ---\n",
            "                                                 Feature  Coefficient\n",
            "1                                      artist_popularity     3.198540\n",
            "8488    artist_popularity key speechiness acousticness^2    -1.381437\n",
            "13210             key speechiness acousticness^2 valence    -0.833017\n",
            "10148           danceability key^2 loudness acousticness     0.766045\n",
            "7953                     artist_popularity energy^3 mode     0.662273\n",
            "0                                                   year     0.660466\n",
            "14642                            mode liveness^3 valence     0.637924\n",
            "9524            danceability^2 key acousticness liveness     0.634513\n",
            "9415                       danceability^2 energy^2 tempo    -0.604943\n",
            "8873   artist_popularity mode speechiness acousticness^2     0.585747\n",
            "9321                         danceability^4 acousticness     0.582358\n",
            "12876                        key loudness mode valence^2     0.569584\n",
            "8400   artist_popularity key loudness speechiness aco...     0.558311\n",
            "12108                   energy mode acousticness tempo^2    -0.549542\n",
            "10636                danceability loudness duration_ms^3     0.544805\n",
            "\n",
            "Características con coeficiente cercano a cero (potencialmente eliminadas por Lasso): 15295\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "\n",
        "# Ignorar advertencias para mantener la salida limpia (opcional)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "df = data.copy()\n",
        "\n",
        "# --- 2. Preprocesamiento de Datos ---\n",
        "\n",
        "# Seleccionar características (features) y variable objetivo (target)\n",
        "features = [\n",
        "    'year', 'artist_popularity', 'danceability', 'energy', 'key',\n",
        "    'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness',\n",
        "    'liveness', 'valence', 'tempo', 'duration_ms', 'time_signature'\n",
        "]\n",
        "target = 'track_popularity'\n",
        "\n",
        "# Verificar si todas las columnas necesarias existen\n",
        "missing_cols = [col for col in features + [target] if col not in df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"\\nError: Faltan las siguientes columnas en el dataset: {missing_cols}\")\n",
        "    exit()\n",
        "\n",
        "# Eliminar filas con valores faltantes en características o target (estrategia simple)\n",
        "# Puedes cambiar esto por imputación si lo prefieres\n",
        "print(f\"\\nForma original: {df.shape}\")\n",
        "df_cleaned = df.dropna(subset=features + [target])\n",
        "print(f\"Forma después de eliminar NaNs: {df_cleaned.shape}\")\n",
        "print(f\"Se eliminaron {df.shape[0] - df_cleaned.shape[0]} filas con valores faltantes.\")\n",
        "\n",
        "if df_cleaned.empty:\n",
        "    print(\"\\nError: El dataset quedó vacío después de eliminar filas con NaNs.\")\n",
        "    exit()\n",
        "\n",
        "X = df_cleaned[features]\n",
        "y = df_cleaned[target]\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTamaño del conjunto de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Tamaño del conjunto de prueba: {X_test.shape}\")\n",
        "\n",
        "# --- 3. Modelo Base: Regresión Lineal (Grado Polinómico 1) ---\n",
        "print(\"\\n--- Modelo Base: Regresión Lineal Simple ---\")\n",
        "# Usamos un pipeline para escalar y luego aplicar regresión lineal\n",
        "# Aunque no es estrictamente necesario para LR simple, mantiene la consistencia\n",
        "base_lr_pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('linear_regression', LinearRegression())\n",
        "])\n",
        "\n",
        "base_lr_pipe.fit(X_train, y_train)\n",
        "y_pred_lr_base = base_lr_pipe.predict(X_test)\n",
        "mse_lr_base = mean_squared_error(y_test, y_pred_lr_base)\n",
        "r2_lr_base = r2_score(y_test, y_pred_lr_base)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse_lr_base:.4f}\")\n",
        "print(f\"R-squared (R²): {r2_lr_base:.4f}\")\n",
        "\n",
        "# --- 4. Función para Buscar el Mejor Modelo Polinómico Regularizado ---\n",
        "\n",
        "def find_best_polynomial_regularized_model(X_train, y_train, X_test, y_test, degrees, alphas):\n",
        "    \"\"\"\n",
        "    Prueba modelos Lasso y Ridge con diferentes grados polinómicos y alphas.\n",
        "\n",
        "    Args:\n",
        "        X_train: Características de entrenamiento (sin escalar).\n",
        "        y_train: Variable objetivo de entrenamiento.\n",
        "        X_test: Características de prueba (sin escalar).\n",
        "        y_test: Variable objetivo de prueba.\n",
        "        degrees: Lista o iterable de grados polinómicos a probar (ej: range(1, 11)).\n",
        "        alphas: Lista o iterable de valores alpha a probar (ej: [0.1, 1, 10]).\n",
        "\n",
        "    Returns:\n",
        "        Un diccionario con la información del mejor modelo encontrado.\n",
        "    \"\"\"\n",
        "    best_model_info = {\n",
        "        'model_type': None,\n",
        "        'degree': None,\n",
        "        'alpha': None,\n",
        "        'best_r2_score': -np.inf, # Inicializar R² con valor muy bajo\n",
        "        'best_mse': np.inf,      # Inicializar MSE con valor muy alto\n",
        "        'best_pipeline': None    # Guardaremos el pipeline completo\n",
        "    }\n",
        "\n",
        "    print(\"\\n--- Buscando Mejor Modelo Polinómico Regularizado ---\")\n",
        "    print(f\"Probando Grados Polinómicos: {list(degrees)}\")\n",
        "    print(f\"Probando Alphas: {list(alphas)}\")\n",
        "\n",
        "    for degree in degrees:\n",
        "        print(f\"\\n--- Probando Grado Polinómico: {degree} ---\")\n",
        "\n",
        "        # Crear el transformador de características polinómicas\n",
        "        # include_bias=False porque el modelo lineal (Lasso/Ridge) ya maneja la intercepción\n",
        "        poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "\n",
        "        # Probar Ridge con este grado\n",
        "        for alpha in alphas:\n",
        "            # Crear el pipeline completo: Poly -> Scaler -> Ridge\n",
        "            ridge_pipe = Pipeline([\n",
        "                ('poly', poly_features),\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('ridge', Ridge(alpha=alpha, random_state=42))\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                # Entrenar el pipeline\n",
        "                ridge_pipe.fit(X_train, y_train)\n",
        "                # Predecir en el conjunto de prueba\n",
        "                y_pred = ridge_pipe.predict(X_test)\n",
        "                # Evaluar\n",
        "                r2 = r2_score(y_test, y_pred)\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                print(f\"  Grado={degree}, Ridge(alpha={alpha:.2f}): R²={r2:.4f}, MSE={mse:.4f}\")\n",
        "\n",
        "                # Actualizar si es el mejor modelo hasta ahora\n",
        "                if r2 > best_model_info['best_r2_score']:\n",
        "                    best_model_info['model_type'] = 'Ridge'\n",
        "                    best_model_info['degree'] = degree\n",
        "                    best_model_info['alpha'] = alpha\n",
        "                    best_model_info['best_r2_score'] = r2\n",
        "                    best_model_info['best_mse'] = mse\n",
        "                    best_model_info['best_pipeline'] = ridge_pipe\n",
        "\n",
        "            except MemoryError:\n",
        "                 print(f\"  Grado={degree}, Ridge(alpha={alpha:.2f}): Error de Memoria - Demasiadas características polinómicas. Saltando.\")\n",
        "                 # Si un grado causa error de memoria, probablemente los siguientes también lo harán para este alpha\n",
        "                 # Podríamos romper el bucle interno de alpha aquí si es necesario\n",
        "                 continue # Pasar al siguiente alpha o grado\n",
        "            except Exception as e:\n",
        "                 print(f\"  Grado={degree}, Ridge(alpha={alpha:.2f}): Error inesperado - {e}. Saltando.\")\n",
        "                 continue\n",
        "\n",
        "        # Probar Lasso con este grado\n",
        "        for alpha in alphas:\n",
        "             # Crear el pipeline completo: Poly -> Scaler -> Lasso\n",
        "            lasso_pipe = Pipeline([\n",
        "                ('poly', poly_features),\n",
        "                ('scaler', StandardScaler()),\n",
        "                ('lasso', Lasso(alpha=alpha, random_state=42, max_iter=10000, tol=0.01)) # Aumentar max_iter y tol puede ayudar a la convergencia\n",
        "            ])\n",
        "\n",
        "            try:\n",
        "                # Entrenar el pipeline\n",
        "                lasso_pipe.fit(X_train, y_train)\n",
        "                # Predecir en el conjunto de prueba\n",
        "                y_pred = lasso_pipe.predict(X_test)\n",
        "                # Evaluar\n",
        "                r2 = r2_score(y_test, y_pred)\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                print(f\"  Grado={degree}, Lasso(alpha={alpha:.2f}): R²={r2:.4f}, MSE={mse:.4f}\")\n",
        "\n",
        "                # Actualizar si es el mejor modelo hasta ahora\n",
        "                if r2 > best_model_info['best_r2_score']:\n",
        "                    best_model_info['model_type'] = 'Lasso'\n",
        "                    best_model_info['degree'] = degree\n",
        "                    best_model_info['alpha'] = alpha\n",
        "                    best_model_info['best_r2_score'] = r2\n",
        "                    best_model_info['best_mse'] = mse\n",
        "                    best_model_info['best_pipeline'] = lasso_pipe\n",
        "\n",
        "            except MemoryError:\n",
        "                 print(f\"  Grado={degree}, Lasso(alpha={alpha:.2f}): Error de Memoria - Demasiadas características polinómicas. Saltando.\")\n",
        "                 continue\n",
        "            except Exception as e:\n",
        "                 print(f\"  Grado={degree}, Lasso(alpha={alpha:.2f}): Error inesperado - {e}. Saltando.\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "    print(\"\\n--- Mejor Modelo Polinómico Regularizado Encontrado ---\")\n",
        "    if best_model_info['best_pipeline']:\n",
        "        print(f\"Tipo de Modelo: {best_model_info['model_type']}\")\n",
        "        print(f\"Grado Polinómico: {best_model_info['degree']}\")\n",
        "        print(f\"Mejor Alpha: {best_model_info['alpha']}\")\n",
        "        print(f\"Mejor R² (en test): {best_model_info['best_r2_score']:.4f}\")\n",
        "        print(f\"Mejor MSE (en test): {best_model_info['best_mse']:.4f}\")\n",
        "    else:\n",
        "        print(\"No se encontró un modelo que mejorara la inicialización (posiblemente debido a errores).\")\n",
        "\n",
        "    return best_model_info\n",
        "\n",
        "# --- 5. Ejecutar la Búsqueda ---\n",
        "\n",
        "# Definir los rangos a probar según tu solicitud\n",
        "degrees_to_test = range(1, 11) # Grados del 1 al 10\n",
        "alphas_to_test = [0.1, 1.0, 10.0] # Alphas 0.1, 1, 10\n",
        "\n",
        "# Ejecutar la función de búsqueda\n",
        "# Pasamos los datos SIN escalar, ya que el escalado está dentro del pipeline\n",
        "best_poly_reg_model_info = find_best_polynomial_regularized_model(\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    degrees_to_test, alphas_to_test\n",
        ")\n",
        "\n",
        "# --- 6. Resumen Final ---\n",
        "print(\"\\n--- Resumen Final de Rendimiento (en conjunto de prueba) ---\")\n",
        "print(f\"Regresión Lineal Simple (Grado 1): R²={r2_lr_base:.4f}, MSE={mse_lr_base:.4f}\")\n",
        "\n",
        "if best_poly_reg_model_info['best_pipeline']:\n",
        "    print(f\"Mejor Modelo Encontrado ({best_poly_reg_model_info['model_type']}, Grado={best_poly_reg_model_info['degree']}, Alpha={best_poly_reg_model_info['alpha']}): R²={best_poly_reg_model_info['best_r2_score']:.4f}, MSE={best_poly_reg_model_info['best_mse']:.4f}\")\n",
        "    improvement_r2 = best_poly_reg_model_info['best_r2_score'] - r2_lr_base\n",
        "    print(f\"Mejora en R² respecto a Regresión Lineal Simple: {improvement_r2:+.4f}\")\n",
        "else:\n",
        "    print(\"No se encontró un modelo polinómico regularizado óptimo en la búsqueda (comparado con R²=-inf).\")\n",
        "\n",
        "# --- 7. (Opcional) Análisis de Coeficientes del Mejor Modelo ---\n",
        "# La interpretación es más compleja con características polinómicas\n",
        "if best_poly_reg_model_info['best_pipeline']:\n",
        "    best_pipeline = best_poly_reg_model_info['best_pipeline']\n",
        "    try:\n",
        "        # Obtener los nombres de las características polinómicas generadas\n",
        "        poly_step = best_pipeline.named_steps['poly']\n",
        "        model_step = best_pipeline.steps[-1][1] # El último paso es el modelo (Lasso o Ridge)\n",
        "\n",
        "        feature_names_poly = poly_step.get_feature_names_out(input_features=features)\n",
        "\n",
        "        coefficients = pd.DataFrame({\n",
        "            'Feature': feature_names_poly,\n",
        "            'Coefficient': model_step.coef_\n",
        "        })\n",
        "        coefficients['Abs_Coefficient'] = coefficients['Coefficient'].abs()\n",
        "        coefficients = coefficients.sort_values(by='Abs_Coefficient', ascending=False).drop('Abs_Coefficient', axis=1)\n",
        "\n",
        "        print(f\"\\n--- Coeficientes del Mejor Modelo ({best_poly_reg_model_info['model_type']}, Grado={best_poly_reg_model_info['degree']}, Alpha={best_poly_reg_model_info['alpha']}) ---\")\n",
        "        print(coefficients.head(15)) # Mostrar solo los N más importantes por claridad\n",
        "\n",
        "        if best_poly_reg_model_info['model_type'] == 'Lasso':\n",
        "            zero_coeffs = coefficients[np.isclose(coefficients['Coefficient'], 0)] # Usar isclose por precisión flotante\n",
        "            print(f\"\\nCaracterísticas con coeficiente cercano a cero (potencialmente eliminadas por Lasso): {len(zero_coeffs)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nNo se pudieron extraer los coeficientes del mejor modelo: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yPaUvmnh8md"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast # Para convertir string de lista a lista real\n",
        "from collections import Counter # Para contar frecuencias de géneros\n",
        "import warnings\n",
        "\n",
        "# Preprocesamiento y Modelado\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, KBinsDiscretizer\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# --- Configuración Inicial ---\n",
        "# Ignorar advertencias (opcional pero útil para salidas limpias)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "df = data.copy()\n",
        "\n",
        "# Seleccionar columnas relevantes iniciales (incluyendo genres)\n",
        "initial_features = [\n",
        "    'track_popularity', 'year', 'artist_popularity', 'danceability',\n",
        "    'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness',\n",
        "    'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms',\n",
        "    'time_signature', 'artist_genres' # Incluimos géneros\n",
        "]\n",
        "\n",
        "existing_features = [col for col in initial_features if col in df.columns]\n",
        "if 'track_popularity' not in existing_features:\n",
        "    print(\"Error: La columna objetivo 'track_popularity' no se encuentra.\")\n",
        "    exit()\n",
        "print(f\"\\nColumnas iniciales seleccionadas: {existing_features}\")\n",
        "\n",
        "df_subset = df[existing_features].copy()\n",
        "\n",
        "# Convertir columnas numéricas (excluyendo genres por ahora)\n",
        "numeric_cols_to_check = [col for col in existing_features if col != 'artist_genres' and col != 'track_popularity']\n",
        "for col in numeric_cols_to_check:\n",
        "    df_subset[col] = pd.to_numeric(df_subset[col], errors='coerce')\n",
        "\n",
        "# Manejar valores faltantes en columnas numéricas y target (eliminar filas)\n",
        "cols_to_check_na = [col for col in existing_features if col != 'artist_genres']\n",
        "initial_rows = df_subset.shape[0]\n",
        "print(f\"\\nValores faltantes ANTES de limpiar (en subset inicial):\\n{df_subset[cols_to_check_na].isnull().sum().sort_values(ascending=False)}\")\n",
        "df_subset.dropna(subset=cols_to_check_na, inplace=True)\n",
        "print(f\"Se eliminaron {initial_rows - df_subset.shape[0]} filas debido a NaNs en columnas numéricas/target.\")\n",
        "print(f\"Forma del dataset DESPUÉS de limpiar NaNs numéricos: {df_subset.shape}\")\n",
        "\n",
        "# Asegurarse de que 'artist_genres' sea string y manejar NaNs (reemplazar con '[]')\n",
        "if 'artist_genres' in df_subset.columns:\n",
        "    df_subset['artist_genres'] = df_subset['artist_genres'].fillna('[]').astype(str)\n",
        "else:\n",
        "    print(\"Advertencia: La columna 'artist_genres' no se encontró.\")\n",
        "\n",
        "if df_subset.empty:\n",
        "    print(\"Error: El dataset quedó vacío después de la limpieza inicial.\")\n",
        "    exit()\n",
        "\n",
        "# Separar X e y TEMPORALMENTE para aplicar ingeniería solo a X\n",
        "X_temp = df_subset.drop('track_popularity', axis=1)\n",
        "y = df_subset['track_popularity'].copy() # y ya está limpio y listo\n",
        "\n",
        "print(\"\\n--- Iniciando Ingeniería de Características ---\")\n",
        "\n",
        "# Crear una copia para no modificar X_temp directamente en cada paso\n",
        "X_engineered = X_temp.copy()\n",
        "\n",
        "# --- 2.1 Procesamiento de Géneros (artist_genres) ---\n",
        "if 'artist_genres' in X_engineered.columns:\n",
        "    print(\"\\nProcesando 'artist_genres'...\")\n",
        "    # Función segura para convertir string de lista a lista\n",
        "    def parse_genre_list(genres_str):\n",
        "        try:\n",
        "            genres = ast.literal_eval(genres_str)\n",
        "            return genres if isinstance(genres, list) else []\n",
        "        except (ValueError, SyntaxError):\n",
        "            return []\n",
        "\n",
        "    X_engineered['genre_list'] = X_engineered['artist_genres'].apply(parse_genre_list)\n",
        "    all_genres = [genre for sublist in X_engineered['genre_list'] for genre in sublist]\n",
        "    genre_counts = Counter(all_genres)\n",
        "    N_TOP_GENRES = 20 # Puedes ajustar esto\n",
        "    top_genres = [genre for genre, count in genre_counts.most_common(N_TOP_GENRES)]\n",
        "    print(f\"Top {N_TOP_GENRES} géneros encontrados (se crearán columnas OHE): {top_genres}\")\n",
        "\n",
        "    for genre in top_genres:\n",
        "        col_name = f'genre_{genre.replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"&\",\"and\").replace(\"'\",\"\")}' # Limpiar nombres\n",
        "        X_engineered[col_name] = X_engineered['genre_list'].apply(lambda x: 1 if genre in x else 0)\n",
        "\n",
        "    X_engineered.drop(['artist_genres', 'genre_list'], axis=1, inplace=True)\n",
        "    print(f\"Se añadieron {len(top_genres)} columnas de género (One-Hot Encoded).\")\n",
        "else:\n",
        "    print(\"\\n'artist_genres' no presente, saltando procesamiento de géneros.\")\n",
        "\n",
        "\n",
        "# --- 2.2 Transformaciones No Lineales ---\n",
        "print(\"\\nAplicando transformaciones no lineales...\")\n",
        "log_features = ['duration_ms', 'speechiness', 'liveness', 'instrumentalness']\n",
        "for col in log_features:\n",
        "    if col in X_engineered.columns:\n",
        "        if (X_engineered[col] < 0).any():\n",
        "            print(f\"Advertencia: La columna '{col}' contiene valores negativos. No se aplicará log.\")\n",
        "        else:\n",
        "            X_engineered[f'{col}_log'] = np.log1p(X_engineered[col])\n",
        "            print(f\" - Columna '{col}_log' creada.\")\n",
        "            # X_engineered.drop(col, axis=1, inplace=True) # Decidimos mantener la original\n",
        "\n",
        "poly_features = ['artist_popularity', 'danceability', 'loudness', 'energy', 'valence']\n",
        "for col in poly_features:\n",
        "     if col in X_engineered.columns:\n",
        "        X_engineered[f'{col}_sq'] = X_engineered[col]**2\n",
        "        print(f\" - Columna '{col}_sq' creada.\")\n",
        "        # X_engineered.drop(col, axis=1, inplace=True) # Mantenemos la original\n",
        "\n",
        "\n",
        "# --- 2.3 Creación de Interacciones ---\n",
        "print(\"\\nCreando características de interacción...\")\n",
        "interactions = [\n",
        "    ('danceability', 'energy'),\n",
        "    ('valence', 'energy'),\n",
        "    # ('artist_popularity', 'acousticness') # Añade o quita según creas necesario\n",
        "]\n",
        "for col1, col2 in interactions:\n",
        "    if col1 in X_engineered.columns and col2 in X_engineered.columns:\n",
        "        X_engineered[f'{col1}_x_{col2}'] = X_engineered[col1] * X_engineered[col2]\n",
        "        print(f\" - Interacción '{col1}_x_{col2}' creada.\")\n",
        "\n",
        "# --- 2.4 Binning (Agrupación) ---\n",
        "print(\"\\nAplicando binning a 'artist_popularity'...\")\n",
        "if 'artist_popularity' in X_engineered.columns:\n",
        "    N_BINS = 5\n",
        "    bin_col_name = 'artist_pop_binned' # Nombre más corto\n",
        "    try:\n",
        "        # Usar solo valores finitos para el discretizer\n",
        "        valid_artist_pop = X_engineered[['artist_popularity']].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "        if not valid_artist_pop.empty:\n",
        "            discretizer = KBinsDiscretizer(n_bins=N_BINS, encode='ordinal', strategy='quantile', subsample=None) # subsample=None para usar todos los datos\n",
        "            # Asegurar que los índices coincidan después de fit_transform\n",
        "            binned_values = discretizer.fit_transform(valid_artist_pop)\n",
        "            binned_series = pd.Series(binned_values.flatten(), index=valid_artist_pop.index, name=bin_col_name)\n",
        "\n",
        "            # Unir los binned values de vuelta al dataframe principal X_engineered\n",
        "            X_engineered = X_engineered.join(binned_series)\n",
        "            print(f\" - Columna ordinal '{bin_col_name}' creada con {N_BINS} bins.\")\n",
        "\n",
        "            # Llenar NaNs en la columna binarizada (si la popularidad original era NaN) con un valor (e.g., -1 o la media/mediana del bin)\n",
        "            X_engineered[bin_col_name].fillna(-1, inplace=True) # Usar -1 para categoría 'missing/otro'\n",
        "\n",
        "            # One-Hot Encode la columna binarizada (incluyendo el -1 si existe)\n",
        "            ohe_binned = pd.get_dummies(X_engineered[bin_col_name], prefix=bin_col_name, dummy_na=False) # dummy_na=False si ya llenaste NaNs\n",
        "            X_engineered = pd.concat([X_engineered, ohe_binned], axis=1)\n",
        "            X_engineered.drop(bin_col_name, axis=1, inplace=True) # Eliminar columna ordinal\n",
        "            print(f\" - Columnas One-Hot creadas para '{bin_col_name}'.\")\n",
        "\n",
        "            # Eliminar la 'artist_popularity' original\n",
        "            if 'artist_popularity' in X_engineered.columns:\n",
        "                 X_engineered.drop('artist_popularity', axis=1, inplace=True)\n",
        "                 print(f\" - Columna 'artist_popularity' original eliminada.\")\n",
        "        else:\n",
        "            print(\" - No hay valores válidos en 'artist_popularity' para binarizar. Saltando binning.\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "         print(f\"Error durante el binning de 'artist_popularity' (posiblemente pocos datos o valores repetidos): {ve}. Saltando este paso.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error inesperado durante el binning de 'artist_popularity': {e}. Saltando este paso.\")\n",
        "\n",
        "else:\n",
        "    print(\"'artist_popularity' no encontrada, saltando binning.\")\n",
        "\n",
        "\n",
        "# --- 2.5 Finalización de Ingeniería ---\n",
        "print(\"\\n--- Ingeniería de Características Completada ---\")\n",
        "\n",
        "# Eliminar columnas que puedan contener infinitos o NaNs residuales\n",
        "X_engineered.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "if X_engineered.isnull().sum().sum() > 0:\n",
        "    print(f\"\\nValores NaN encontrados DESPUÉS de ingeniería:\\n{X_engineered.isnull().sum()[X_engineered.isnull().sum() > 0]}\")\n",
        "    print(\"Imputando NaNs restantes con la mediana de cada columna...\")\n",
        "    for col in X_engineered.columns[X_engineered.isnull().any()]:\n",
        "        median_val = X_engineered[col].median()\n",
        "        X_engineered[col].fillna(median_val, inplace=True)\n",
        "\n",
        "# Asegurarse de que todas las columnas sean numéricas\n",
        "non_numeric_cols = X_engineered.select_dtypes(exclude=np.number).columns\n",
        "if len(non_numeric_cols) > 0:\n",
        "    print(f\"\\nAdvertencia: Se encontraron columnas no numéricas al final: {list(non_numeric_cols)}. Intentando convertir...\")\n",
        "    for col in non_numeric_cols:\n",
        "        X_engineered[col] = pd.to_numeric(X_engineered[col], errors='coerce')\n",
        "    # Volver a imputar si la conversión creó NaNs\n",
        "    if X_engineered.isnull().sum().sum() > 0:\n",
        "         for col in X_engineered.columns[X_engineered.isnull().any()]:\n",
        "            median_val = X_engineered[col].median()\n",
        "            X_engineered[col].fillna(median_val, inplace=True)\n",
        "\n",
        "print(f\"\\nForma final de X (características ingenierizadas): {X_engineered.shape}\")\n",
        "print(f\"Número de características finales: {X_engineered.shape[1]}\")\n",
        "# print(\"\\nColumnas finales en X_engineered:\") # Descomentar si quieres ver la lista completa\n",
        "# print(X_engineered.columns.tolist())\n",
        "\n",
        "# --- 3. División Train/Test (DESPUÉS de la ingeniería) ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_engineered, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\n--- Datos divididos para modelado ---\")\n",
        "print(f\"Tamaño Entrenamiento: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Tamaño Prueba: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "\n",
        "# --- 4. Definición de Pipelines y Búsqueda de Hiperparámetros ---\n",
        "\n",
        "# Rangos de hiperparámetros a probar\n",
        "degrees = list(range(2, 11)) # Grados polinómicos de 2 a 10\n",
        "alphas = [0.1, 1.0, 10.0]    # Alphas para Lasso y Ridge\n",
        "\n",
        "# Crear pipelines (Scaler -> Poly -> Regressor)\n",
        "# NOTA: PolynomialFeatures con muchas características de entrada puede ser MUY costoso computacionalmente.\n",
        "# Si el proceso es demasiado lento, considera reducir el número de características de entrada o el rango de 'degrees'.\n",
        "\n",
        "# Pipeline para Lasso\n",
        "pipeline_lasso = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(include_bias=False)),\n",
        "    ('regressor', Lasso(random_state=42, max_iter=15000)) # Aumentar max_iter puede ser necesario\n",
        "])\n",
        "\n",
        "# Pipeline para Ridge\n",
        "pipeline_ridge = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('poly', PolynomialFeatures(include_bias=False)),\n",
        "    ('regressor', Ridge(random_state=42, solver='auto')) # 'auto' elige el solver más eficiente\n",
        "])\n",
        "\n",
        "# Parámetros para GridSearchCV\n",
        "param_grid = {\n",
        "    'poly__degree': degrees,\n",
        "    'regressor__alpha': alphas\n",
        "}\n",
        "\n",
        "# Configurar Validación Cruzada\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# --- Ejecutar GridSearchCV para Lasso ---\n",
        "print(\"\\n--- Iniciando búsqueda de hiperparámetros para Regresión Polinómica con Lasso ---\")\n",
        "grid_search_lasso = GridSearchCV(\n",
        "    pipeline_lasso,\n",
        "    param_grid,\n",
        "    cv=cv,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1, # Usar todos los procesadores\n",
        "    verbose=1\n",
        ")\n",
        "grid_search_lasso.fit(X_train, y_train) # Entrenar sobre el conjunto de entrenamiento\n",
        "\n",
        "print(\"\\n--- Resultados de GridSearchCV para Lasso ---\")\n",
        "print(f\"Mejor puntuación (neg MSE): {grid_search_lasso.best_score_:.4f}\")\n",
        "best_rmse_cv_lasso = np.sqrt(-grid_search_lasso.best_score_)\n",
        "print(f\"Mejor RMSE (Validación Cruzada): {best_rmse_cv_lasso:.4f}\")\n",
        "print(f\"Mejores parámetros: {grid_search_lasso.best_params_}\")\n",
        "\n",
        "# --- Ejecutar GridSearchCV para Ridge ---\n",
        "print(\"\\n--- Iniciando búsqueda de hiperparámetros para Regresión Polinómica con Ridge ---\")\n",
        "grid_search_ridge = GridSearchCV(\n",
        "    pipeline_ridge,\n",
        "    param_grid, # Usamos el mismo grid de grados y alphas\n",
        "    cv=cv,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search_ridge.fit(X_train, y_train) # Entrenar sobre el conjunto de entrenamiento\n",
        "\n",
        "print(\"\\n--- Resultados de GridSearchCV para Ridge ---\")\n",
        "print(f\"Mejor puntuación (neg MSE): {grid_search_ridge.best_score_:.4f}\")\n",
        "best_rmse_cv_ridge = np.sqrt(-grid_search_ridge.best_score_)\n",
        "print(f\"Mejor RMSE (Validación Cruzada): {best_rmse_cv_ridge:.4f}\")\n",
        "print(f\"Mejores parámetros: {grid_search_ridge.best_params_}\")\n",
        "\n",
        "\n",
        "# --- 5. Selección del Mejor Modelo y Evaluación Final ---\n",
        "print(\"\\n--- Comparando Modelos Finales ---\")\n",
        "\n",
        "best_model = None\n",
        "best_model_name = \"\"\n",
        "best_params = None\n",
        "best_cv_rmse = float('inf')\n",
        "\n",
        "if best_rmse_cv_lasso < best_rmse_cv_ridge:\n",
        "    print(f\"El mejor modelo (basado en CV RMSE) es Lasso Polinómico con RMSE = {best_rmse_cv_lasso:.4f}\")\n",
        "    best_model = grid_search_lasso.best_estimator_\n",
        "    best_model_name = \"Lasso Polinómico\"\n",
        "    best_params = grid_search_lasso.best_params_\n",
        "    best_cv_rmse = best_rmse_cv_lasso\n",
        "else:\n",
        "    print(f\"El mejor modelo (basado en CV RMSE) es Ridge Polinómico con RMSE = {best_rmse_cv_ridge:.4f}\")\n",
        "    best_model = grid_search_ridge.best_estimator_\n",
        "    best_model_name = \"Ridge Polinómico\"\n",
        "    best_params = grid_search_ridge.best_params_\n",
        "    best_cv_rmse = best_rmse_cv_ridge\n",
        "\n",
        "print(f\"\\nModelo seleccionado final: {best_model_name}\")\n",
        "print(f\"Mejores Hiperparámetros encontrados:\")\n",
        "print(f\"  Grado Polinómico: {best_params['poly__degree']}\")\n",
        "print(f\"  Alpha de Regularización: {best_params['regressor__alpha']}\")\n",
        "print(f\"RMSE estimado (Validación Cruzada): {best_cv_rmse:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Evaluando el MEJOR modelo en el CONJUNTO DE PRUEBA ---\")\n",
        "\n",
        "# Predecir en el conjunto de prueba usando el mejor pipeline encontrado\n",
        "y_pred_test = best_model.predict(X_test)\n",
        "\n",
        "# Calcular métricas en el conjunto de prueba\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "test_r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"\\nResultados en el Conjunto de Prueba:\")\n",
        "print(f\"  RMSE: {test_rmse:.4f}\")\n",
        "print(f\"  R²: {test_r2:.4f}\")\n",
        "\n",
        "# (Opcional) Comparar con RMSE base (predecir siempre la media)\n",
        "# base_rmse = np.sqrt(mean_squared_error(y_test, [y_train.mean()] * len(y_test)))\n",
        "# print(f\"  RMSE Baseline (predecir media): {base_rmse:.4f}\")\n",
        "\n",
        "print(f\"\\nRango de 'track_popularity' (Target): {y.min()} - {y.max()}\")\n",
        "print(f\"Desviación estándar de 'track_popularity' en Test Set: {y_test.std():.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Proceso completado ---\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOdU0yiSpURI52/AQMB5gQ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}